master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Overriding config with config/train_whatsapp.py:Overriding config with config/train_whatsapp.py:

# config for training GPT-2 (124M) down to very nice loss of ~2.85 on 1 node of 8X A100 40GB
# launch as the following (e.g. in a screen session) and wait ~5 days:
# $ torchrun --standalone --nproc_per_node=8 train.py config/train_whatsapp.py

#####################################################################
# CHANGED default parameters from train.py model:
#####################################################################
out_dir = 'out-whatsapp'
dataset = 'whatsapp'
wandb_log = False
wandb_project = 'whatsapp'
wandb_run_name='gpt2-124M'

# bias = False
min_lr = 1e-4 # learning_rate / 10 usually
beta2 = 0.99 # make a bit bigger because number of tokens per iter is small

# eval stuff
eval_interval = 250 # keep frequent because we'll overfit
eval_iters = 200
log_interval = 10 # don't print too too often

#####################################################################
# UNCHANGED default parameters from train.py model:
#####################################################################
# these make the total batch size be ~0.5M (recalculate for whatsapp)
# 12 batch size * 1024 block size * 5 gradaccum * 8 GPUs = 491,520 (recalculate for whatsapp)
batch_size = 12
block_size = 1024
gradient_accumulation_steps = 5 * 8

# this makes total number of tokens be 300B (recalculate for whatsapp)
max_iters = 600000
lr_decay_iters = 600000

# weight decay
weight_decay = 1e-1


#tokens per iteration will be: 491,520
#number of parameters: 123.59M
# config for training GPT-2 (124M) down to very nice loss of ~2.85 on 1 node of 8X A100 40GB
# launch as the following (e.g. in a screen session) and wait ~5 days:
# $ torchrun --standalone --nproc_per_node=8 train.py config/train_whatsapp.py

#####################################################################
# CHANGED default parameters from train.py model:
#####################################################################
out_dir = 'out-whatsapp'
dataset = 'whatsapp'
wandb_log = False
wandb_project = 'whatsapp'
wandb_run_name='gpt2-124M'

# bias = False
min_lr = 1e-4 # learning_rate / 10 usually
beta2 = 0.99 # make a bit bigger because number of tokens per iter is small

# eval stuff
eval_interval = 250 # keep frequent because we'll overfit
eval_iters = 200
log_interval = 10 # don't print too too often

#####################################################################
# UNCHANGED default parameters from train.py model:
#####################################################################
# these make the total batch size be ~0.5M (recalculate for whatsapp)
# 12 batch size * 1024 block size * 5 gradaccum * 8 GPUs = 491,520 (recalculate for whatsapp)
batch_size = 12
block_size = 1024
gradient_accumulation_steps = 5 * 8

# this makes total number of tokens be 300B (recalculate for whatsapp)
max_iters = 600000
lr_decay_iters = 600000

# weight decay
weight_decay = 1e-1


#tokens per iteration will be: 491,520
#number of parameters: 123.59M
Overriding config with config/train_whatsapp.py:
# config for training GPT-2 (124M) down to very nice loss of ~2.85 on 1 node of 8X A100 40GB
# launch as the following (e.g. in a screen session) and wait ~5 days:
# $ torchrun --standalone --nproc_per_node=8 train.py config/train_whatsapp.py

#####################################################################
# CHANGED default parameters from train.py model:
#####################################################################
out_dir = 'out-whatsapp'
dataset = 'whatsapp'
wandb_log = False
wandb_project = 'whatsapp'
wandb_run_name='gpt2-124M'

# bias = False
min_lr = 1e-4 # learning_rate / 10 usually
beta2 = 0.99 # make a bit bigger because number of tokens per iter is small

# eval stuff
eval_interval = 250 # keep frequent because we'll overfit
eval_iters = 200
log_interval = 10 # don't print too too often

#####################################################################
# UNCHANGED default parameters from train.py model:
#####################################################################
# these make the total batch size be ~0.5M (recalculate for whatsapp)
# 12 batch size * 1024 block size * 5 gradaccum * 8 GPUs = 491,520 (recalculate for whatsapp)
batch_size = 12
block_size = 1024
gradient_accumulation_steps = 5 * 8

# this makes total number of tokens be 300B (recalculate for whatsapp)
max_iters = 600000
lr_decay_iters = 600000

# weight decay
weight_decay = 1e-1


#tokens per iteration will be: 491,520
#number of parameters: 123.59M
Overriding config with config/train_whatsapp.py:
# config for training GPT-2 (124M) down to very nice loss of ~2.85 on 1 node of 8X A100 40GB
# launch as the following (e.g. in a screen session) and wait ~5 days:
# $ torchrun --standalone --nproc_per_node=8 train.py config/train_whatsapp.py

#####################################################################
# CHANGED default parameters from train.py model:
#####################################################################
out_dir = 'out-whatsapp'
dataset = 'whatsapp'
wandb_log = False
wandb_project = 'whatsapp'
wandb_run_name='gpt2-124M'

# bias = False
min_lr = 1e-4 # learning_rate / 10 usually
beta2 = 0.99 # make a bit bigger because number of tokens per iter is small

# eval stuff
eval_interval = 250 # keep frequent because we'll overfit
eval_iters = 200
log_interval = 10 # don't print too too often

#####################################################################
# UNCHANGED default parameters from train.py model:
#####################################################################
# these make the total batch size be ~0.5M (recalculate for whatsapp)
# 12 batch size * 1024 block size * 5 gradaccum * 8 GPUs = 491,520 (recalculate for whatsapp)
batch_size = 12
block_size = 1024
gradient_accumulation_steps = 5 * 8

# this makes total number of tokens be 300B (recalculate for whatsapp)
max_iters = 600000
lr_decay_iters = 600000

# weight decay
weight_decay = 1e-1


#tokens per iteration will be: 491,520
#number of parameters: 123.59M
tokens per iteration will be: 491,520
tokens per iteration will be: 491,520tokens per iteration will be: 491,520

tokens per iteration will be: 491,520
Initializing a new model from scratch
Initializing a new model from scratchInitializing a new model from scratchdefaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)
Initializing a new model from scratch

defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)
defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)
defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)

number of parameters: 123.59M
number of parameters: 123.59M
number of parameters: 123.59M
number of parameters: 123.59M
num decayed parameter tensors: 50, with 124,354,560 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
using fused AdamW: True
compiling the model... (takes a ~minute)
num decayed parameter tensors: 50, with 124,354,560 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
using fused AdamW: True
compiling the model... (takes a ~minute)
num decayed parameter tensors: 50, with 124,354,560 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
using fused AdamW: True
compiling the model... (takes a ~minute)
num decayed parameter tensors: 50, with 124,354,560 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
using fused AdamW: True
compiling the model... (takes a ~minute)
/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.3) or chardet (5.1.0) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.3) or chardet (5.1.0) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.3) or chardet (5.1.0) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.3) or chardet (5.1.0) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
step 0: train loss 10.9786, val loss 10.9765
iter 0: loss 10.9807, time 34819.17ms, mfu -100.00%
iter 10: loss 10.2626, time 835.46ms, mfu 40.29%
iter 20: loss 9.2857, time 835.31ms, mfu 40.30%
iter 30: loss 8.9212, time 837.02ms, mfu 40.29%
iter 40: loss 8.3362, time 838.05ms, mfu 40.28%
iter 50: loss 7.6895, time 837.76ms, mfu 40.27%
iter 60: loss 7.3525, time 838.64ms, mfu 40.25%
iter 70: loss 6.8220, time 838.38ms, mfu 40.24%
iter 80: loss 6.5586, time 838.47ms, mfu 40.24%
iter 90: loss 6.5405, time 839.38ms, mfu 40.22%
iter 100: loss 6.0499, time 839.77ms, mfu 40.21%
iter 110: loss 6.0185, time 839.22ms, mfu 40.20%
iter 120: loss 5.5227, time 839.40ms, mfu 40.19%
iter 130: loss 5.2670, time 839.22ms, mfu 40.18%
iter 140: loss 5.0093, time 838.97ms, mfu 40.18%
iter 150: loss 4.7852, time 839.87ms, mfu 40.17%
iter 160: loss 4.6014, time 839.37ms, mfu 40.16%
iter 170: loss 4.0930, time 839.06ms, mfu 40.16%
iter 180: loss 4.3871, time 840.11ms, mfu 40.15%
iter 190: loss 3.9806, time 840.99ms, mfu 40.14%
iter 200: loss 3.9256, time 842.02ms, mfu 40.12%
iter 210: loss 3.3142, time 839.88ms, mfu 40.12%
iter 220: loss 3.5063, time 839.86ms, mfu 40.11%
iter 230: loss 3.5292, time 841.02ms, mfu 40.11%
iter 240: loss 3.3870, time 840.00ms, mfu 40.10%
step 250: train loss 3.4197, val loss 4.2895
saving checkpoint to out-whatsapp
iter 250: loss 3.1609, time 11662.01ms, mfu 36.38%
iter 260: loss 3.2681, time 838.06ms, mfu 36.76%
iter 270: loss 3.1930, time 839.40ms, mfu 37.09%
iter 280: loss 3.4704, time 841.15ms, mfu 37.39%
iter 290: loss 3.2240, time 840.13ms, mfu 37.66%
iter 300: loss 3.3175, time 840.75ms, mfu 37.89%
iter 310: loss 3.1754, time 839.65ms, mfu 38.11%
iter 320: loss 3.0036, time 841.29ms, mfu 38.30%
iter 330: loss 3.1429, time 842.19ms, mfu 38.47%
iter 340: loss 3.1980, time 841.06ms, mfu 38.63%
iter 350: loss 2.9396, time 839.46ms, mfu 38.77%
iter 360: loss 3.0103, time 841.42ms, mfu 38.90%
iter 370: loss 3.1691, time 840.07ms, mfu 39.02%
iter 380: loss 2.8426, time 841.57ms, mfu 39.11%
iter 390: loss 2.8958, time 842.24ms, mfu 39.20%
iter 400: loss 2.8318, time 841.06ms, mfu 39.28%
iter 410: loss 2.7530, time 840.68ms, mfu 39.36%
iter 420: loss 2.5889, time 840.24ms, mfu 39.43%
iter 430: loss 2.9440, time 840.27ms, mfu 39.49%
iter 440: loss 2.6920, time 840.80ms, mfu 39.55%
iter 450: loss 2.5605, time 842.12ms, mfu 39.59%
iter 460: loss 2.6536, time 841.80ms, mfu 39.63%
iter 470: loss 2.5595, time 841.35ms, mfu 39.67%
iter 480: loss 2.9449, time 841.55ms, mfu 39.70%
iter 490: loss 2.6315, time 842.32ms, mfu 39.73%
step 500: train loss 2.5501, val loss 3.5086
saving checkpoint to out-whatsapp
iter 500: loss 2.5356, time 11650.11ms, mfu 36.04%
iter 510: loss 2.5027, time 839.42ms, mfu 36.45%
iter 520: loss 2.4596, time 840.20ms, mfu 36.81%
iter 530: loss 2.3043, time 839.36ms, mfu 37.14%
iter 540: loss 2.4416, time 840.87ms, mfu 37.43%
iter 550: loss 2.1939, time 839.61ms, mfu 37.70%
iter 560: loss 2.4246, time 839.34ms, mfu 37.94%
iter 570: loss 2.4088, time 839.79ms, mfu 38.15%
iter 580: loss 2.3598, time 842.31ms, mfu 38.34%
iter 590: loss 2.3005, time 840.77ms, mfu 38.51%
iter 600: loss 2.1852, time 840.53ms, mfu 38.66%
iter 610: loss 2.2066, time 841.89ms, mfu 38.79%
iter 620: loss 2.1570, time 842.04ms, mfu 38.91%
iter 630: loss 2.2322, time 840.47ms, mfu 39.03%
iter 640: loss 2.2303, time 840.70ms, mfu 39.13%
iter 650: loss 2.2287, time 842.15ms, mfu 39.21%
iter 660: loss 2.0955, time 842.46ms, mfu 39.29%
iter 670: loss 2.1543, time 839.66ms, mfu 39.37%
iter 680: loss 1.9711, time 840.18ms, mfu 39.44%
iter 690: loss 2.0168, time 842.67ms, mfu 39.49%
iter 700: loss 1.9668, time 841.96ms, mfu 39.54%
iter 710: loss 1.9864, time 842.19ms, mfu 39.58%
iter 720: loss 1.7350, time 840.78ms, mfu 39.63%
iter 730: loss 1.6807, time 842.53ms, mfu 39.66%
iter 740: loss 1.7794, time 842.11ms, mfu 39.69%
step 750: train loss 1.7479, val loss 3.4929
saving checkpoint to out-whatsapp
iter 750: loss 1.6914, time 11778.46ms, mfu 36.01%
iter 760: loss 1.7207, time 840.88ms, mfu 36.41%
iter 770: loss 1.6203, time 842.03ms, mfu 36.77%
iter 780: loss 1.5569, time 842.17ms, mfu 37.09%
iter 790: loss 1.5727, time 839.72ms, mfu 37.39%
iter 800: loss 1.3225, time 842.04ms, mfu 37.65%
iter 810: loss 1.3645, time 842.37ms, mfu 37.88%
iter 820: loss 1.4007, time 842.05ms, mfu 38.09%
iter 830: loss 1.3486, time 842.12ms, mfu 38.28%
iter 840: loss 1.3837, time 841.62ms, mfu 38.45%
iter 850: loss 1.2044, time 842.13ms, mfu 38.60%
iter 860: loss 1.1192, time 841.71ms, mfu 38.74%
iter 870: loss 1.0633, time 841.30ms, mfu 38.87%
iter 880: loss 0.9932, time 842.53ms, mfu 38.98%
iter 890: loss 1.0328, time 840.75ms, mfu 39.08%
iter 900: loss 0.8162, time 846.90ms, mfu 39.15%
iter 910: loss 0.8789, time 842.36ms, mfu 39.23%
iter 920: loss 0.7988, time 841.59ms, mfu 39.31%
iter 930: loss 0.8185, time 841.27ms, mfu 39.38%
iter 940: loss 0.7057, time 848.69ms, mfu 39.41%
iter 950: loss 0.6730, time 842.79ms, mfu 39.46%
iter 960: loss 0.6735, time 840.86ms, mfu 39.52%
iter 970: loss 0.5834, time 841.26ms, mfu 39.57%
iter 980: loss 0.4974, time 841.33ms, mfu 39.61%
iter 990: loss 0.4791, time 842.08ms, mfu 39.65%
step 1000: train loss 0.4662, val loss 4.5907
saving checkpoint to out-whatsapp
iter 1000: loss 0.4535, time 11726.64ms, mfu 35.97%
iter 1010: loss 0.4650, time 839.10ms, mfu 36.39%
iter 1020: loss 0.4461, time 841.55ms, mfu 36.75%
iter 1030: loss 0.3663, time 842.23ms, mfu 37.07%
iter 1040: loss 0.3587, time 841.14ms, mfu 37.37%
iter 1050: loss 0.2914, time 841.68ms, mfu 37.63%
iter 1060: loss 0.3150, time 842.11ms, mfu 37.86%
iter 1070: loss 0.2417, time 842.42ms, mfu 38.07%
iter 1080: loss 0.2356, time 842.28ms, mfu 38.26%
iter 1090: loss 0.2433, time 842.18ms, mfu 38.43%
iter 1100: loss 0.2240, time 841.98ms, mfu 38.59%
iter 1110: loss 0.2222, time 840.77ms, mfu 38.73%
iter 1120: loss 0.2273, time 841.56ms, mfu 38.86%
iter 1130: loss 0.2149, time 841.37ms, mfu 38.98%
iter 1140: loss 0.3293, time 841.86ms, mfu 39.08%
iter 1150: loss 0.2556, time 841.99ms, mfu 39.17%
iter 1160: loss 0.7782, time 841.66ms, mfu 39.25%
iter 1170: loss 5.7645, time 845.76ms, mfu 39.31%
iter 1180: loss 3.6350, time 840.46ms, mfu 39.38%
iter 1190: loss 2.6764, time 841.18ms, mfu 39.45%
iter 1200: loss 2.3629, time 841.84ms, mfu 39.50%
iter 1210: loss 2.1662, time 841.73ms, mfu 39.55%
iter 1220: loss 2.0649, time 840.60ms, mfu 39.60%
iter 1230: loss 1.6075, time 841.65ms, mfu 39.64%
iter 1240: loss 1.7036, time 842.27ms, mfu 39.67%
step 1250: train loss 1.3459, val loss 3.9747
saving checkpoint to out-whatsapp
iter 1250: loss 1.4343, time 11722.71ms, mfu 35.99%
iter 1260: loss 1.1994, time 838.31ms, mfu 36.41%
iter 1270: loss 0.8660, time 841.08ms, mfu 36.77%
iter 1280: loss 0.6574, time 840.50ms, mfu 37.10%
iter 1290: loss 0.3753, time 840.87ms, mfu 37.39%
iter 1300: loss 0.2782, time 840.89ms, mfu 37.66%
iter 1310: loss 0.2229, time 841.82ms, mfu 37.89%
iter 1320: loss 0.1452, time 840.47ms, mfu 38.11%
iter 1330: loss 0.1733, time 842.63ms, mfu 38.29%
iter 1340: loss 0.1454, time 843.97ms, mfu 38.45%
iter 1350: loss 0.1559, time 843.05ms, mfu 38.60%
iter 1360: loss 0.1614, time 842.37ms, mfu 38.74%
iter 1370: loss 0.1510, time 842.43ms, mfu 38.86%
iter 1380: loss 0.1121, time 841.98ms, mfu 38.97%
iter 1390: loss 0.1462, time 841.97ms, mfu 39.07%
iter 1400: loss 0.0953, time 842.92ms, mfu 39.16%
iter 1410: loss 0.0976, time 841.75ms, mfu 39.24%
iter 1420: loss 0.1067, time 842.65ms, mfu 39.31%
iter 1430: loss 0.0904, time 842.15ms, mfu 39.38%
iter 1440: loss 0.1003, time 841.16ms, mfu 39.44%
iter 1450: loss 0.0973, time 842.56ms, mfu 39.49%
iter 1460: loss 0.0878, time 841.28ms, mfu 39.55%
iter 1470: loss 0.0972, time 840.89ms, mfu 39.60%
iter 1480: loss 0.0756, time 841.86ms, mfu 39.63%
iter 1490: loss 0.0886, time 841.86ms, mfu 39.67%
step 1500: train loss 0.1047, val loss 5.4705
saving checkpoint to out-whatsapp
iter 1500: loss 0.1034, time 11726.83ms, mfu 35.99%
iter 1510: loss 0.1776, time 839.93ms, mfu 36.40%
iter 1520: loss 0.1138, time 840.00ms, mfu 36.77%
iter 1530: loss 0.1011, time 841.95ms, mfu 37.09%
iter 1540: loss 0.0942, time 840.63ms, mfu 37.38%
iter 1550: loss 0.0817, time 840.57ms, mfu 37.65%
iter 1560: loss 0.0636, time 840.58ms, mfu 37.89%
iter 1570: loss 0.0621, time 842.49ms, mfu 38.10%
iter 1580: loss 0.0682, time 841.39ms, mfu 38.29%
iter 1590: loss 0.0772, time 840.81ms, mfu 38.46%
iter 1600: loss 0.0734, time 842.52ms, mfu 38.61%
iter 1610: loss 0.1681, time 842.24ms, mfu 38.75%
iter 1620: loss 0.3206, time 840.70ms, mfu 38.88%
iter 1630: loss 0.4458, time 842.28ms, mfu 38.99%
iter 1640: loss 0.6007, time 843.90ms, mfu 39.08%
iter 1650: loss 2.8966, time 841.24ms, mfu 39.17%
iter 1660: loss 6.2988, time 840.36ms, mfu 39.26%
iter 1670: loss 4.4827, time 841.93ms, mfu 39.33%
iter 1680: loss 3.1527, time 841.04ms, mfu 39.40%
iter 1690: loss 2.9063, time 841.71ms, mfu 39.46%
iter 1700: loss 2.4903, time 841.92ms, mfu 39.51%
iter 1710: loss 2.2948, time 842.19ms, mfu 39.56%
iter 1720: loss 2.1143, time 842.47ms, mfu 39.60%
iter 1730: loss 1.7877, time 842.61ms, mfu 39.64%
iter 1740: loss 1.4424, time 841.34ms, mfu 39.67%
step 1750: train loss 1.0810, val loss 4.1790
saving checkpoint to out-whatsapp
iter 1750: loss 1.0709, time 11725.38ms, mfu 35.99%
iter 1760: loss 0.7990, time 839.03ms, mfu 36.41%
iter 1770: loss 0.4791, time 838.82ms, mfu 36.78%
iter 1780: loss 0.4702, time 841.66ms, mfu 37.10%
iter 1790: loss 0.3027, time 840.38ms, mfu 37.40%
iter 1800: loss 0.1409, time 841.47ms, mfu 37.66%
iter 1810: loss 0.0886, time 841.61ms, mfu 37.89%
iter 1820: loss 0.0608, time 842.25ms, mfu 38.10%
iter 1830: loss 0.0545, time 841.05ms, mfu 38.29%
iter 1840: loss 0.0611, time 841.30ms, mfu 38.46%
iter 1850: loss 0.0693, time 843.33ms, mfu 38.61%
iter 1860: loss 0.0594, time 841.53ms, mfu 38.75%
iter 1870: loss 0.0581, time 842.04ms, mfu 38.87%
iter 1880: loss 0.0650, time 840.96ms, mfu 38.99%
iter 1890: loss 0.0579, time 842.11ms, mfu 39.09%
iter 1900: loss 0.0634, time 841.66ms, mfu 39.18%
iter 1910: loss 0.0579, time 843.19ms, mfu 39.25%
iter 1920: loss 0.0606, time 843.30ms, mfu 39.32%
iter 1930: loss 0.0713, time 841.25ms, mfu 39.39%
iter 1940: loss 0.0618, time 841.30ms, mfu 39.45%
iter 1950: loss 0.1670, time 843.45ms, mfu 39.50%
iter 1960: loss 0.1281, time 842.32ms, mfu 39.55%
iter 1970: loss 0.0876, time 842.05ms, mfu 39.59%
iter 1980: loss 0.0762, time 842.47ms, mfu 39.63%
iter 1990: loss 0.0698, time 841.79ms, mfu 39.66%
step 2000: train loss 0.0680, val loss 5.8644
saving checkpoint to out-whatsapp
iter 2000: loss 0.0598, time 11733.41ms, mfu 35.98%
iter 2010: loss 0.0780, time 839.43ms, mfu 36.40%
iter 2020: loss 0.0717, time 840.44ms, mfu 36.76%
iter 2030: loss 0.0655, time 841.43ms, mfu 37.09%
iter 2040: loss 0.0654, time 840.95ms, mfu 37.38%
iter 2050: loss 0.0715, time 842.99ms, mfu 37.64%
iter 2060: loss 0.0578, time 842.95ms, mfu 37.87%
iter 2070: loss 0.0631, time 842.59ms, mfu 38.07%
iter 2080: loss 0.0604, time 841.30ms, mfu 38.27%
iter 2090: loss 0.0692, time 841.53ms, mfu 38.44%
iter 2100: loss 0.1390, time 841.64ms, mfu 38.60%
iter 2110: loss 0.1805, time 841.81ms, mfu 38.74%
iter 2120: loss 0.1122, time 841.84ms, mfu 38.86%
iter 2130: loss 0.0815, time 841.70ms, mfu 38.98%
iter 2140: loss 0.0654, time 841.93ms, mfu 39.08%
iter 2150: loss 0.0675, time 840.92ms, mfu 39.17%
iter 2160: loss 0.0675, time 842.50ms, mfu 39.25%
iter 2170: loss 0.0604, time 843.50ms, mfu 39.32%
iter 2180: loss 0.0495, time 841.84ms, mfu 39.38%
iter 2190: loss 0.0499, time 842.03ms, mfu 39.44%
iter 2200: loss 0.0416, time 841.40ms, mfu 39.50%
iter 2210: loss 0.0523, time 841.93ms, mfu 39.55%
iter 2220: loss 0.0517, time 842.55ms, mfu 39.59%
iter 2230: loss 0.0483, time 842.01ms, mfu 39.63%
iter 2240: loss 0.0558, time 842.05ms, mfu 39.66%
step 2250: train loss 0.0529, val loss 6.2481
saving checkpoint to out-whatsapp
iter 2250: loss 0.0534, time 11731.46ms, mfu 35.98%
iter 2260: loss 0.0548, time 839.65ms, mfu 36.40%
iter 2270: loss 0.0577, time 841.81ms, mfu 36.76%
iter 2280: loss 0.0611, time 842.14ms, mfu 37.08%
iter 2290: loss 0.0642, time 840.61ms, mfu 37.37%
iter 2300: loss 0.0562, time 841.03ms, mfu 37.64%
iter 2310: loss 0.2799, time 840.25ms, mfu 37.88%
iter 2320: loss 0.5055, time 841.03ms, mfu 38.10%
iter 2330: loss 0.3418, time 841.55ms, mfu 38.29%
iter 2340: loss 2.4594, time 840.70ms, mfu 38.46%
iter 2350: loss 2.1152, time 842.34ms, mfu 38.61%
iter 2360: loss 1.7093, time 840.10ms, mfu 38.76%
iter 2370: loss 1.1816, time 842.12ms, mfu 38.88%
iter 2380: loss 0.8811, time 842.63ms, mfu 38.99%
iter 2390: loss 0.3555, time 841.58ms, mfu 39.09%
iter 2400: loss 0.1247, time 842.00ms, mfu 39.18%
iter 2410: loss 0.0811, time 841.64ms, mfu 39.26%
iter 2420: loss 0.0506, time 841.94ms, mfu 39.33%
iter 2430: loss 0.0431, time 841.91ms, mfu 39.40%
iter 2440: loss 0.0412, time 842.66ms, mfu 39.45%
iter 2450: loss 0.0369, time 841.18ms, mfu 39.51%
iter 2460: loss 0.0334, time 841.84ms, mfu 39.56%
iter 2470: loss 0.0403, time 841.57ms, mfu 39.60%
iter 2480: loss 0.0378, time 842.78ms, mfu 39.64%
iter 2490: loss 0.0329, time 841.96ms, mfu 39.67%
step 2500: train loss 0.0365, val loss 6.3056
saving checkpoint to out-whatsapp
iter 2500: loss 0.0435, time 11731.78ms, mfu 35.99%
iter 2510: loss 0.0449, time 838.45ms, mfu 36.41%
iter 2520: loss 0.0370, time 838.80ms, mfu 36.78%
iter 2530: loss 0.0348, time 843.06ms, mfu 37.10%
iter 2540: loss 0.0391, time 841.18ms, mfu 37.39%
iter 2550: loss 0.0378, time 840.70ms, mfu 37.65%
iter 2560: loss 0.0336, time 841.30ms, mfu 37.89%
iter 2570: loss 0.0396, time 841.63ms, mfu 38.10%
iter 2580: loss 0.0479, time 841.92ms, mfu 38.29%
iter 2590: loss 0.0438, time 841.22ms, mfu 38.46%
iter 2600: loss 0.0367, time 841.42ms, mfu 38.62%
iter 2610: loss 0.0358, time 842.71ms, mfu 38.75%
iter 2620: loss 0.0418, time 841.24ms, mfu 38.88%
iter 2630: loss 0.0431, time 841.90ms, mfu 38.99%
iter 2640: loss 0.0419, time 841.10ms, mfu 39.09%
iter 2650: loss 0.0488, time 840.98ms, mfu 39.19%
iter 2660: loss 0.0508, time 840.96ms, mfu 39.27%
iter 2670: loss 0.0513, time 842.25ms, mfu 39.34%
iter 2680: loss 0.0439, time 841.55ms, mfu 39.41%
iter 2690: loss 0.0451, time 842.52ms, mfu 39.46%
iter 2700: loss 0.0591, time 841.78ms, mfu 39.51%
iter 2710: loss 0.1639, time 841.75ms, mfu 39.56%
iter 2720: loss 0.2461, time 843.05ms, mfu 39.60%
iter 2730: loss 0.8389, time 841.42ms, mfu 39.64%
iter 2740: loss 1.5407, time 840.46ms, mfu 39.68%
step 2750: train loss 2.0456, val loss 4.4351
saving checkpoint to out-whatsapp
iter 2750: loss 2.1999, time 11725.35ms, mfu 36.00%
iter 2760: loss 1.9137, time 838.38ms, mfu 36.42%
iter 2770: loss 1.2455, time 840.55ms, mfu 36.78%
iter 2780: loss 0.3263, time 841.93ms, mfu 37.10%
iter 2790: loss 0.0938, time 841.18ms, mfu 37.39%
iter 2800: loss 0.0642, time 840.91ms, mfu 37.66%
iter 2810: loss 0.0418, time 841.03ms, mfu 37.89%
iter 2820: loss 0.0410, time 840.09ms, mfu 38.11%
iter 2830: loss 0.0366, time 841.80ms, mfu 38.30%
iter 2840: loss 0.0292, time 840.88ms, mfu 38.47%
iter 2850: loss 0.0369, time 842.47ms, mfu 38.62%
iter 2860: loss 0.0360, time 842.31ms, mfu 38.76%
iter 2870: loss 0.0336, time 842.10ms, mfu 38.88%
iter 2880: loss 0.0406, time 841.89ms, mfu 38.99%
iter 2890: loss 0.0337, time 843.74ms, mfu 39.08%
iter 2900: loss 0.0351, time 843.22ms, mfu 39.16%
iter 2910: loss 0.0310, time 842.02ms, mfu 39.25%
iter 2920: loss 0.0366, time 841.49ms, mfu 39.32%
iter 2930: loss 0.0297, time 841.63ms, mfu 39.39%
iter 2940: loss 0.0313, time 842.48ms, mfu 39.45%
iter 2950: loss 0.0338, time 842.55ms, mfu 39.50%
iter 2960: loss 0.0374, time 842.41ms, mfu 39.54%
iter 2970: loss 0.0366, time 840.92ms, mfu 39.59%
iter 2980: loss 0.0323, time 841.84ms, mfu 39.63%
iter 2990: loss 0.0318, time 842.86ms, mfu 39.66%
step 3000: train loss 0.0349, val loss 6.4920
saving checkpoint to out-whatsapp
iter 3000: loss 0.0321, time 11776.46ms, mfu 35.98%
iter 3010: loss 0.0416, time 839.24ms, mfu 36.40%
iter 3020: loss 0.0403, time 842.27ms, mfu 36.75%
iter 3030: loss 0.0402, time 841.04ms, mfu 37.08%
iter 3040: loss 0.0417, time 842.69ms, mfu 37.37%
iter 3050: loss 0.0474, time 841.12ms, mfu 37.63%
iter 3060: loss 0.0526, time 841.16ms, mfu 37.87%
iter 3070: loss 0.0458, time 841.12ms, mfu 38.09%
iter 3080: loss 0.0457, time 841.99ms, mfu 38.28%
iter 3090: loss 0.0454, time 841.57ms, mfu 38.45%
iter 3100: loss 0.0453, time 842.51ms, mfu 38.60%
iter 3110: loss 0.0473, time 842.53ms, mfu 38.74%
iter 3120: loss 0.0414, time 842.17ms, mfu 38.86%
iter 3130: loss 0.0416, time 841.76ms, mfu 38.97%
iter 3140: loss 0.0420, time 842.68ms, mfu 39.07%
iter 3150: loss 0.0422, time 842.12ms, mfu 39.16%
iter 3160: loss 0.0418, time 840.75ms, mfu 39.25%
iter 3170: loss 0.0432, time 841.69ms, mfu 39.32%
iter 3180: loss 0.0413, time 842.44ms, mfu 39.39%
iter 3190: loss 0.0456, time 841.93ms, mfu 39.45%
iter 3200: loss 0.0426, time 840.88ms, mfu 39.51%
iter 3210: loss 0.0455, time 842.32ms, mfu 39.55%
iter 3220: loss 0.0443, time 841.58ms, mfu 39.60%
iter 3230: loss 0.0406, time 842.29ms, mfu 39.63%
iter 3240: loss 0.0483, time 844.57ms, mfu 39.66%
step 3250: train loss 0.0422, val loss 6.7655
saving checkpoint to out-whatsapp
iter 3250: loss 0.0375, time 11733.52ms, mfu 35.98%
iter 3260: loss 0.0350, time 839.47ms, mfu 36.39%
iter 3270: loss 0.0504, time 839.77ms, mfu 36.76%
iter 3280: loss 0.0429, time 839.62ms, mfu 37.09%
iter 3290: loss 0.0465, time 838.74ms, mfu 37.40%
iter 3300: loss 0.0366, time 842.04ms, mfu 37.66%
iter 3310: loss 0.0418, time 843.26ms, mfu 37.88%
iter 3320: loss 0.0544, time 842.01ms, mfu 38.09%
iter 3330: loss 0.0366, time 840.67ms, mfu 38.29%
iter 3340: loss 0.0448, time 840.98ms, mfu 38.46%
iter 3350: loss 0.0463, time 842.92ms, mfu 38.61%
iter 3360: loss 0.0401, time 844.11ms, mfu 38.74%
iter 3370: loss 0.0329, time 840.98ms, mfu 38.87%
iter 3380: loss 0.0400, time 842.14ms, mfu 38.98%
iter 3390: loss 0.0364, time 841.68ms, mfu 39.08%
iter 3400: loss 0.0345, time 841.77ms, mfu 39.17%
iter 3410: loss 0.0394, time 841.99ms, mfu 39.25%
iter 3420: loss 0.0341, time 841.83ms, mfu 39.33%
iter 3430: loss 0.0407, time 841.78ms, mfu 39.39%
iter 3440: loss 0.0356, time 841.66ms, mfu 39.45%
iter 3450: loss 0.0410, time 842.09ms, mfu 39.51%
iter 3460: loss 0.0382, time 841.56ms, mfu 39.56%
iter 3470: loss 0.0402, time 841.29ms, mfu 39.60%
iter 3480: loss 0.0387, time 842.11ms, mfu 39.64%
iter 3490: loss 0.0376, time 841.23ms, mfu 39.68%
step 3500: train loss 0.0381, val loss 7.0423
saving checkpoint to out-whatsapp
iter 3500: loss 0.0344, time 11775.34ms, mfu 35.99%
iter 3510: loss 0.0356, time 840.27ms, mfu 36.40%
iter 3520: loss 0.0392, time 839.98ms, mfu 36.77%
iter 3530: loss 0.0403, time 843.09ms, mfu 37.09%
iter 3540: loss 0.0424, time 841.12ms, mfu 37.38%
iter 3550: loss 0.0402, time 840.95ms, mfu 37.64%
iter 3560: loss 0.0349, time 842.12ms, mfu 37.88%
iter 3570: loss 0.0476, time 841.61ms, mfu 38.09%
iter 3580: loss 0.0429, time 842.66ms, mfu 38.28%
iter 3590: loss 0.0435, time 842.19ms, mfu 38.45%
iter 3600: loss 0.0355, time 841.78ms, mfu 38.60%
iter 3610: loss 0.0327, time 842.13ms, mfu 38.74%
iter 3620: loss 0.0374, time 841.68ms, mfu 38.86%
iter 3630: loss 0.0375, time 841.07ms, mfu 38.98%
iter 3640: loss 0.0505, time 839.92ms, mfu 39.09%
iter 3650: loss 0.0285, time 841.22ms, mfu 39.18%
iter 3660: loss 0.0338, time 841.21ms, mfu 39.27%
iter 3670: loss 0.0383, time 841.46ms, mfu 39.34%
iter 3680: loss 0.0336, time 841.39ms, mfu 39.41%
iter 3690: loss 0.0364, time 840.27ms, mfu 39.47%
iter 3700: loss 0.0397, time 841.86ms, mfu 39.52%
iter 3710: loss 0.0382, time 841.88ms, mfu 39.57%
iter 3720: loss 0.0411, time 841.49ms, mfu 39.61%
iter 3730: loss 0.0309, time 840.76ms, mfu 39.66%
iter 3740: loss 0.0347, time 841.17ms, mfu 39.69%
step 3750: train loss 0.0359, val loss 7.1187
saving checkpoint to out-whatsapp
iter 3750: loss 0.0338, time 11733.33ms, mfu 36.01%
iter 3760: loss 0.0335, time 839.02ms, mfu 36.42%
iter 3770: loss 0.0304, time 841.30ms, mfu 36.78%
iter 3780: loss 0.0309, time 842.08ms, mfu 37.10%
iter 3790: loss 0.0386, time 842.78ms, mfu 37.39%
iter 3800: loss 0.0388, time 843.14ms, mfu 37.64%
iter 3810: loss 0.0351, time 841.58ms, mfu 37.88%
iter 3820: loss 0.0418, time 840.91ms, mfu 38.09%
iter 3830: loss 0.0334, time 839.71ms, mfu 38.29%
iter 3840: loss 0.0376, time 840.69ms, mfu 38.47%
iter 3850: loss 0.0394, time 840.49ms, mfu 38.63%
iter 3860: loss 0.1765, time 842.26ms, mfu 38.76%
iter 3870: loss 0.3757, time 841.24ms, mfu 38.89%
iter 3880: loss 0.0669, time 842.26ms, mfu 38.99%
iter 3890: loss 0.0414, time 842.96ms, mfu 39.09%
iter 3900: loss 0.0356, time 843.26ms, mfu 39.17%
iter 3910: loss 0.0320, time 841.97ms, mfu 39.25%
iter 3920: loss 0.0307, time 841.55ms, mfu 39.33%
iter 3930: loss 0.0297, time 840.80ms, mfu 39.40%
iter 3940: loss 0.0226, time 841.67ms, mfu 39.46%
iter 3950: loss 0.0241, time 841.21ms, mfu 39.52%
iter 3960: loss 0.0276, time 841.30ms, mfu 39.57%
iter 3970: loss 0.0239, time 840.24ms, mfu 39.62%
iter 3980: loss 0.0264, time 842.06ms, mfu 39.65%
iter 3990: loss 0.0228, time 843.33ms, mfu 39.68%
step 4000: train loss 0.0262, val loss 7.2972
saving checkpoint to out-whatsapp
iter 4000: loss 0.0256, time 11704.46ms, mfu 36.00%
iter 4010: loss 0.0243, time 839.63ms, mfu 36.41%
iter 4020: loss 0.0279, time 840.22ms, mfu 36.77%
iter 4030: loss 0.0267, time 839.80ms, mfu 37.10%
iter 4040: loss 0.0272, time 842.34ms, mfu 37.39%
iter 4050: loss 0.0280, time 841.83ms, mfu 37.65%
iter 4060: loss 0.0289, time 841.71ms, mfu 37.89%
iter 4070: loss 0.0281, time 840.72ms, mfu 38.10%
iter 4080: loss 0.0370, time 841.82ms, mfu 38.29%
iter 4090: loss 0.0248, time 843.16ms, mfu 38.45%
iter 4100: loss 0.0354, time 840.92ms, mfu 38.61%
iter 4110: loss 0.0303, time 841.66ms, mfu 38.75%
iter 4120: loss 0.0364, time 842.00ms, mfu 38.87%
iter 4130: loss 0.0348, time 843.24ms, mfu 38.98%
iter 4140: loss 0.0329, time 840.74ms, mfu 39.08%
iter 4150: loss 0.0395, time 841.67ms, mfu 39.18%
iter 4160: loss 0.0314, time 842.22ms, mfu 39.26%
iter 4170: loss 0.0305, time 841.08ms, mfu 39.33%
iter 4180: loss 0.0286, time 841.67ms, mfu 39.40%
iter 4190: loss 0.0331, time 842.26ms, mfu 39.46%
iter 4200: loss 0.0410, time 842.98ms, mfu 39.50%
iter 4210: loss 0.0345, time 842.36ms, mfu 39.55%
iter 4220: loss 0.0316, time 840.19ms, mfu 39.60%
iter 4230: loss 0.0306, time 842.06ms, mfu 39.64%
iter 4240: loss 0.0273, time 841.00ms, mfu 39.68%
step 4250: train loss 0.0323, val loss 7.3060
saving checkpoint to out-whatsapp
iter 4250: loss 0.0338, time 11743.85ms, mfu 36.00%
iter 4260: loss 0.0258, time 838.18ms, mfu 36.41%
iter 4270: loss 0.0297, time 839.62ms, mfu 36.78%
iter 4280: loss 0.0332, time 839.97ms, mfu 37.11%
iter 4290: loss 0.0318, time 841.51ms, mfu 37.40%
iter 4300: loss 0.0312, time 841.10ms, mfu 37.66%
iter 4310: loss 0.0320, time 841.40ms, mfu 37.90%
iter 4320: loss 0.0309, time 841.50ms, mfu 38.11%
iter 4330: loss 0.0357, time 842.27ms, mfu 38.30%
iter 4340: loss 0.0302, time 840.13ms, mfu 38.47%
iter 4350: loss 0.0334, time 840.30ms, mfu 38.63%
iter 4360: loss 0.0285, time 840.20ms, mfu 38.78%
iter 4370: loss 0.0283, time 841.24ms, mfu 38.90%
iter 4380: loss 0.0297, time 841.74ms, mfu 39.01%
iter 4390: loss 0.0255, time 841.94ms, mfu 39.11%
iter 4400: loss 0.0291, time 840.82ms, mfu 39.20%
iter 4410: loss 0.0523, time 840.30ms, mfu 39.29%
iter 4420: loss 0.0337, time 841.57ms, mfu 39.36%
iter 4430: loss 0.0356, time 841.22ms, mfu 39.42%
iter 4440: loss 0.0330, time 840.38ms, mfu 39.49%
iter 4450: loss 0.0304, time 842.41ms, mfu 39.53%
iter 4460: loss 0.0230, time 842.56ms, mfu 39.58%
iter 4470: loss 0.0255, time 841.57ms, mfu 39.62%
iter 4480: loss 0.0349, time 841.64ms, mfu 39.66%
iter 4490: loss 0.0307, time 841.94ms, mfu 39.69%
step 4500: train loss 0.0282, val loss 7.5141
saving checkpoint to out-whatsapp
iter 4500: loss 0.0295, time 11937.96ms, mfu 36.00%
iter 4510: loss 0.0261, time 838.73ms, mfu 36.42%
iter 4520: loss 0.0296, time 840.71ms, mfu 36.78%
iter 4530: loss 0.0339, time 840.05ms, mfu 37.11%
iter 4540: loss 0.0283, time 840.47ms, mfu 37.40%
iter 4550: loss 0.0288, time 841.85ms, mfu 37.66%
iter 4560: loss 0.0264, time 840.80ms, mfu 37.90%
iter 4570: loss 0.0331, time 840.53ms, mfu 38.11%
iter 4580: loss 0.0248, time 840.23ms, mfu 38.31%
iter 4590: loss 0.0293, time 841.18ms, mfu 38.48%
iter 4600: loss 0.0294, time 841.25ms, mfu 38.63%
iter 4610: loss 0.0287, time 840.55ms, mfu 38.78%
iter 4620: loss 0.0290, time 841.19ms, mfu 38.90%
iter 4630: loss 0.0297, time 840.91ms, mfu 39.01%
iter 4640: loss 0.0310, time 842.42ms, mfu 39.11%
iter 4650: loss 0.0299, time 840.87ms, mfu 39.20%
iter 4660: loss 0.0321, time 841.72ms, mfu 39.28%
iter 4670: loss 0.0263, time 841.06ms, mfu 39.36%
iter 4680: loss 0.0315, time 841.42ms, mfu 39.42%
iter 4690: loss 0.0301, time 841.40ms, mfu 39.48%
iter 4700: loss 0.0225, time 841.89ms, mfu 39.53%
iter 4710: loss 0.0295, time 839.85ms, mfu 39.59%
iter 4720: loss 0.0321, time 840.71ms, mfu 39.63%
iter 4730: loss 0.0289, time 841.51ms, mfu 39.67%
iter 4740: loss 0.0316, time 841.04ms, mfu 39.70%
step 4750: train loss 0.0300, val loss 7.6364
saving checkpoint to out-whatsapp
iter 4750: loss 0.0308, time 11743.07ms, mfu 36.02%
iter 4760: loss 0.0267, time 840.42ms, mfu 36.42%
iter 4770: loss 0.0319, time 840.31ms, mfu 36.79%
iter 4780: loss 0.0318, time 840.87ms, mfu 37.11%
iter 4790: loss 0.0286, time 841.93ms, mfu 37.40%
iter 4800: loss 0.0358, time 841.86ms, mfu 37.66%
iter 4810: loss 0.0290, time 840.98ms, mfu 37.90%
iter 4820: loss 0.0348, time 842.04ms, mfu 38.10%
iter 4830: loss 0.0256, time 840.96ms, mfu 38.30%
iter 4840: loss 0.0294, time 842.00ms, mfu 38.47%
iter 4850: loss 0.0269, time 841.56ms, mfu 38.62%
iter 4860: loss 0.0282, time 840.52ms, mfu 38.76%
iter 4870: loss 0.0307, time 841.92ms, mfu 38.89%
iter 4880: loss 0.0276, time 843.22ms, mfu 38.99%
iter 4890: loss 0.0261, time 841.45ms, mfu 39.09%
iter 4900: loss 0.0278, time 840.91ms, mfu 39.19%
iter 4910: loss 0.0274, time 842.14ms, mfu 39.26%
iter 4920: loss 0.0245, time 840.92ms, mfu 39.34%
iter 4930: loss 0.0315, time 840.42ms, mfu 39.41%
iter 4940: loss 0.0285, time 841.79ms, mfu 39.47%
iter 4950: loss 0.0255, time 841.94ms, mfu 39.52%
iter 4960: loss 0.0272, time 842.38ms, mfu 39.57%
iter 4970: loss 0.0317, time 841.92ms, mfu 39.61%
iter 4980: loss 0.0295, time 840.79ms, mfu 39.65%
iter 4990: loss 0.0351, time 842.32ms, mfu 39.68%
step 5000: train loss 0.0281, val loss 7.8080
saving checkpoint to out-whatsapp
iter 5000: loss 0.0265, time 11744.26ms, mfu 36.00%
iter 5010: loss 0.0276, time 839.99ms, mfu 36.41%
iter 5020: loss 0.0274, time 839.63ms, mfu 36.78%
iter 5030: loss 0.0267, time 839.71ms, mfu 37.11%
iter 5040: loss 0.0268, time 839.49ms, mfu 37.41%
iter 5050: loss 0.0271, time 841.09ms, mfu 37.67%
iter 5060: loss 0.0272, time 840.20ms, mfu 37.91%
iter 5070: loss 0.0320, time 841.68ms, mfu 38.12%
iter 5080: loss 0.0333, time 841.71ms, mfu 38.31%
iter 5090: loss 0.0282, time 840.67ms, mfu 38.48%
iter 5100: loss 0.0247, time 840.15ms, mfu 38.64%
iter 5110: loss 0.0302, time 842.52ms, mfu 38.77%
iter 5120: loss 0.0239, time 841.47ms, mfu 38.89%
iter 5130: loss 0.0200, time 842.08ms, mfu 39.00%
iter 5140: loss 0.0249, time 842.44ms, mfu 39.10%
iter 5150: loss 0.0255, time 840.71ms, mfu 39.19%
iter 5160: loss 0.0280, time 840.90ms, mfu 39.28%
iter 5170: loss 0.0263, time 841.28ms, mfu 39.35%
iter 5180: loss 0.0283, time 840.95ms, mfu 39.42%
iter 5190: loss 0.0283, time 841.02ms, mfu 39.48%
iter 5200: loss 0.0275, time 840.86ms, mfu 39.54%
iter 5210: loss 0.0264, time 841.55ms, mfu 39.58%
iter 5220: loss 0.0236, time 840.27ms, mfu 39.63%
iter 5230: loss 0.0255, time 840.61ms, mfu 39.67%
iter 5240: loss 0.0290, time 840.80ms, mfu 39.71%
step 5250: train loss 0.0271, val loss 7.9611
saving checkpoint to out-whatsapp
iter 5250: loss 0.0338, time 11743.90ms, mfu 36.02%
iter 5260: loss 0.0282, time 837.79ms, mfu 36.44%
iter 5270: loss 0.0303, time 839.55ms, mfu 36.81%
iter 5280: loss 0.0268, time 840.14ms, mfu 37.13%
iter 5290: loss 0.0293, time 842.44ms, mfu 37.42%
iter 5300: loss 0.0211, time 839.49ms, mfu 37.68%
iter 5310: loss 0.0315, time 840.11ms, mfu 37.92%
iter 5320: loss 0.0275, time 841.89ms, mfu 38.13%
iter 5330: loss 0.0249, time 841.19ms, mfu 38.32%
iter 5340: loss 0.0251, time 840.24ms, mfu 38.49%
iter 5350: loss 0.0269, time 840.46ms, mfu 38.65%
iter 5360: loss 0.0246, time 841.65ms, mfu 38.78%
iter 5370: loss 0.0293, time 840.93ms, mfu 38.91%
iter 5380: loss 0.0259, time 839.77ms, mfu 39.03%
iter 5390: loss 0.0266, time 840.17ms, mfu 39.13%
iter 5400: loss 0.0293, time 841.11ms, mfu 39.22%
iter 5410: loss 0.0267, time 840.88ms, mfu 39.30%
iter 5420: loss 0.0232, time 840.29ms, mfu 39.38%
iter 5430: loss 0.0284, time 840.34ms, mfu 39.45%
iter 5440: loss 0.0281, time 841.53ms, mfu 39.50%
iter 5450: loss 0.0261, time 841.22ms, mfu 39.55%
iter 5460: loss 0.0251, time 840.82ms, mfu 39.60%
iter 5470: loss 0.0292, time 839.40ms, mfu 39.65%
iter 5480: loss 0.0280, time 839.44ms, mfu 39.70%
iter 5490: loss 0.0236, time 841.04ms, mfu 39.73%
step 5500: train loss 0.0268, val loss 8.1709
saving checkpoint to out-whatsapp
iter 5500: loss 0.0278, time 11740.28ms, mfu 36.04%
iter 5510: loss 0.0291, time 839.36ms, mfu 36.45%
iter 5520: loss 0.0287, time 839.64ms, mfu 36.82%
iter 5530: loss 0.0248, time 841.68ms, mfu 37.13%
iter 5540: loss 0.0273, time 841.28ms, mfu 37.42%
iter 5550: loss 0.0256, time 840.49ms, mfu 37.68%
iter 5560: loss 0.0264, time 840.68ms, mfu 37.92%
iter 5570: loss 0.0270, time 842.50ms, mfu 38.12%
iter 5580: loss 0.0266, time 840.68ms, mfu 38.32%
iter 5590: loss 0.0247, time 841.68ms, mfu 38.48%
iter 5600: loss 0.0245, time 842.64ms, mfu 38.63%
iter 5610: loss 0.0234, time 840.38ms, mfu 38.77%
iter 5620: loss 0.0255, time 841.09ms, mfu 38.90%
iter 5630: loss 0.0258, time 841.73ms, mfu 39.01%
iter 5640: loss 0.0298, time 841.76ms, mfu 39.11%
iter 5650: loss 0.0258, time 840.41ms, mfu 39.20%
iter 5660: loss 0.0235, time 840.87ms, mfu 39.29%
iter 5670: loss 0.0256, time 843.73ms, mfu 39.35%
iter 5680: loss 0.0261, time 841.45ms, mfu 39.41%
iter 5690: loss 0.0240, time 841.71ms, mfu 39.47%
iter 5700: loss 0.0231, time 840.09ms, mfu 39.53%
iter 5710: loss 0.0240, time 840.56ms, mfu 39.58%
iter 5720: loss 0.0229, time 841.70ms, mfu 39.62%
iter 5730: loss 0.0233, time 840.84ms, mfu 39.67%
iter 5740: loss 0.0277, time 839.58ms, mfu 39.71%
step 5750: train loss 0.0251, val loss 8.1785
saving checkpoint to out-whatsapp
iter 5750: loss 0.0242, time 11737.32ms, mfu 36.02%
iter 5760: loss 0.0229, time 839.26ms, mfu 36.43%
iter 5770: loss 0.0240, time 839.68ms, mfu 36.80%
iter 5780: loss 0.0257, time 839.21ms, mfu 37.13%
iter 5790: loss 0.0270, time 840.18ms, mfu 37.42%
iter 5800: loss 0.0288, time 840.92ms, mfu 37.69%
iter 5810: loss 0.0276, time 839.86ms, mfu 37.93%
iter 5820: loss 0.0291, time 841.27ms, mfu 38.13%
iter 5830: loss 0.0239, time 839.54ms, mfu 38.33%
iter 5840: loss 0.0299, time 841.27ms, mfu 38.50%
iter 5850: loss 0.0220, time 841.21ms, mfu 38.65%
iter 5860: loss 0.0171, time 841.53ms, mfu 38.79%
iter 5870: loss 0.0231, time 840.70ms, mfu 38.91%
iter 5880: loss 0.0234, time 841.25ms, mfu 39.02%
iter 5890: loss 0.0226, time 840.17ms, mfu 39.13%
iter 5900: loss 0.0251, time 838.80ms, mfu 39.23%
iter 5910: loss 0.0269, time 841.09ms, mfu 39.31%
iter 5920: loss 0.0254, time 841.83ms, mfu 39.38%
iter 5930: loss 0.0234, time 839.72ms, mfu 39.45%
iter 5940: loss 0.0257, time 840.83ms, mfu 39.51%
iter 5950: loss 0.0265, time 842.48ms, mfu 39.55%
iter 5960: loss 0.0262, time 840.55ms, mfu 39.60%
iter 5970: loss 0.0232, time 840.40ms, mfu 39.65%
iter 5980: loss 0.0272, time 840.29ms, mfu 39.69%
iter 5990: loss 0.0273, time 842.00ms, mfu 39.72%
step 6000: train loss 0.0246, val loss 8.2959
saving checkpoint to out-whatsapp
iter 6000: loss 0.0290, time 11745.39ms, mfu 36.03%
iter 6010: loss 0.0294, time 839.88ms, mfu 36.44%
iter 6020: loss 0.0213, time 839.15ms, mfu 36.81%
iter 6030: loss 0.0251, time 841.56ms, mfu 37.13%
iter 6040: loss 0.0298, time 840.81ms, mfu 37.42%
iter 6050: loss 0.0227, time 842.49ms, mfu 37.67%
iter 6060: loss 0.0303, time 840.87ms, mfu 37.91%
iter 6070: loss 0.0207, time 841.00ms, mfu 38.12%
iter 6080: loss 0.0256, time 840.51ms, mfu 38.31%
iter 6090: loss 0.0225, time 840.66ms, mfu 38.49%
iter 6100: loss 0.0236, time 842.37ms, mfu 38.63%
iter 6110: loss 0.0266, time 839.92ms, mfu 38.78%
iter 6120: loss 0.0227, time 841.52ms, mfu 38.90%
iter 6130: loss 0.0249, time 840.56ms, mfu 39.02%
iter 6140: loss 0.0210, time 842.16ms, mfu 39.11%
iter 6150: loss 0.0218, time 840.82ms, mfu 39.20%
iter 6160: loss 0.0243, time 842.11ms, mfu 39.28%
iter 6170: loss 0.0249, time 840.87ms, mfu 39.36%
iter 6180: loss 0.0254, time 840.95ms, mfu 39.42%
iter 6190: loss 0.0275, time 841.80ms, mfu 39.48%
iter 6200: loss 0.0250, time 841.18ms, mfu 39.54%
iter 6210: loss 0.0262, time 842.96ms, mfu 39.58%
iter 6220: loss 0.0194, time 841.84ms, mfu 39.62%
iter 6230: loss 0.0257, time 841.20ms, mfu 39.66%
iter 6240: loss 0.0264, time 841.96ms, mfu 39.69%
step 6250: train loss 0.0240, val loss 8.3432
saving checkpoint to out-whatsapp
iter 6250: loss 0.0230, time 11743.91ms, mfu 36.01%
iter 6260: loss 0.0229, time 838.57ms, mfu 36.42%
iter 6270: loss 0.0274, time 839.58ms, mfu 36.79%
iter 6280: loss 0.0246, time 840.15ms, mfu 37.12%
iter 6290: loss 0.0234, time 841.26ms, mfu 37.41%
iter 6300: loss 0.0229, time 841.14ms, mfu 37.67%
iter 6310: loss 0.0223, time 840.11ms, mfu 37.91%
iter 6320: loss 0.0231, time 841.91ms, mfu 38.12%
iter 6330: loss 0.0255, time 839.67ms, mfu 38.31%
iter 6340: loss 0.0248, time 840.79ms, mfu 38.49%
iter 6350: loss 0.0224, time 841.81ms, mfu 38.64%
iter 6360: loss 0.0198, time 840.56ms, mfu 38.78%
iter 6370: loss 0.0214, time 841.80ms, mfu 38.90%
iter 6380: loss 0.0217, time 840.81ms, mfu 39.01%
iter 6390: loss 0.0220, time 841.57ms, mfu 39.11%
iter 6400: loss 0.0181, time 842.96ms, mfu 39.19%
iter 6410: loss 0.0230, time 840.09ms, mfu 39.28%
iter 6420: loss 0.0207, time 840.27ms, mfu 39.36%
iter 6430: loss 0.0171, time 840.90ms, mfu 39.43%
iter 6440: loss 0.0243, time 840.43ms, mfu 39.49%
iter 6450: loss 0.0212, time 843.08ms, mfu 39.54%
iter 6460: loss 0.0259, time 842.48ms, mfu 39.58%
iter 6470: loss 0.0214, time 840.98ms, mfu 39.62%
iter 6480: loss 0.0217, time 839.78ms, mfu 39.67%
iter 6490: loss 0.0210, time 840.11ms, mfu 39.71%
step 6500: train loss 0.0235, val loss 8.4031
saving checkpoint to out-whatsapp
iter 6500: loss 0.0210, time 11740.96ms, mfu 36.03%
iter 6510: loss 0.0235, time 839.63ms, mfu 36.43%
iter 6520: loss 0.0270, time 841.16ms, mfu 36.79%
iter 6530: loss 0.0288, time 840.15ms, mfu 37.12%
iter 6540: loss 0.0261, time 840.82ms, mfu 37.41%
iter 6550: loss 0.0213, time 839.72ms, mfu 37.68%
iter 6560: loss 0.0219, time 838.97ms, mfu 37.92%
iter 6570: loss 0.0243, time 840.67ms, mfu 38.14%
iter 6580: loss 0.0206, time 841.70ms, mfu 38.32%
iter 6590: loss 0.0209, time 842.16ms, mfu 38.49%
iter 6600: loss 0.0227, time 840.38ms, mfu 38.64%
iter 6610: loss 0.0228, time 841.72ms, mfu 38.78%
iter 6620: loss 0.0267, time 839.95ms, mfu 38.91%
iter 6630: loss 0.0240, time 841.09ms, mfu 39.02%
iter 6640: loss 0.0195, time 840.62ms, mfu 39.12%
iter 6650: loss 0.0229, time 841.03ms, mfu 39.21%
iter 6660: loss 0.0240, time 841.29ms, mfu 39.29%
iter 6670: loss 0.0230, time 840.43ms, mfu 39.37%
iter 6680: loss 0.0213, time 841.14ms, mfu 39.44%
iter 6690: loss 0.0204, time 841.15ms, mfu 39.49%
iter 6700: loss 0.0219, time 840.32ms, mfu 39.55%
iter 6710: loss 0.0232, time 840.32ms, mfu 39.60%
iter 6720: loss 0.0269, time 841.04ms, mfu 39.64%
iter 6730: loss 0.0230, time 841.61ms, mfu 39.68%
iter 6740: loss 0.0204, time 842.23ms, mfu 39.71%
step 6750: train loss 0.0229, val loss 8.6173
saving checkpoint to out-whatsapp
iter 6750: loss 0.0196, time 11781.80ms, mfu 36.02%
iter 6760: loss 0.0223, time 838.46ms, mfu 36.44%
iter 6770: loss 0.0217, time 840.38ms, mfu 36.80%
iter 6780: loss 0.0185, time 839.80ms, mfu 37.13%
iter 6790: loss 0.0214, time 840.01ms, mfu 37.42%
iter 6800: loss 0.0225, time 841.67ms, mfu 37.68%
iter 6810: loss 0.0212, time 840.16ms, mfu 37.92%
iter 6820: loss 0.0275, time 840.09ms, mfu 38.13%
iter 6830: loss 0.0222, time 840.55ms, mfu 38.33%
iter 6840: loss 0.0236, time 840.49ms, mfu 38.50%
iter 6850: loss 0.0273, time 841.23ms, mfu 38.65%
iter 6860: loss 0.0252, time 841.08ms, mfu 38.79%
iter 6870: loss 0.0235, time 839.45ms, mfu 38.92%
iter 6880: loss 0.0197, time 840.26ms, mfu 39.03%
iter 6890: loss 0.0241, time 841.92ms, mfu 39.13%
iter 6900: loss 0.0229, time 840.27ms, mfu 39.22%
iter 6910: loss 0.0226, time 841.32ms, mfu 39.30%
iter 6920: loss 0.0196, time 841.32ms, mfu 39.37%
iter 6930: loss 0.0254, time 841.45ms, mfu 39.44%
iter 6940: loss 0.0210, time 841.32ms, mfu 39.49%
iter 6950: loss 0.0194, time 840.73ms, mfu 39.55%
iter 6960: loss 0.0225, time 839.80ms, mfu 39.60%
iter 6970: loss 0.0205, time 839.79ms, mfu 39.65%
iter 6980: loss 0.0213, time 841.58ms, mfu 39.69%
iter 6990: loss 0.0173, time 841.58ms, mfu 39.72%
step 7000: train loss 0.0222, val loss 8.4974
saving checkpoint to out-whatsapp
iter 7000: loss 0.0197, time 11743.12ms, mfu 36.03%
iter 7010: loss 0.0222, time 837.71ms, mfu 36.45%
iter 7020: loss 0.0213, time 839.45ms, mfu 36.81%
iter 7030: loss 0.0198, time 838.43ms, mfu 37.15%
iter 7040: loss 0.0210, time 839.86ms, mfu 37.44%
iter 7050: loss 0.0242, time 840.48ms, mfu 37.70%
iter 7060: loss 0.0246, time 840.23ms, mfu 37.94%
iter 7070: loss 0.0233, time 838.99ms, mfu 38.16%
iter 7080: loss 0.0248, time 839.91ms, mfu 38.35%
iter 7090: loss 0.0227, time 841.44ms, mfu 38.52%
iter 7100: loss 0.0246, time 841.51ms, mfu 38.66%
iter 7110: loss 0.0248, time 840.31ms, mfu 38.80%
iter 7120: loss 0.0173, time 840.65ms, mfu 38.93%
iter 7130: loss 0.0237, time 840.28ms, mfu 39.04%
iter 7140: loss 0.0229, time 841.03ms, mfu 39.14%
iter 7150: loss 0.0217, time 839.95ms, mfu 39.23%
iter 7160: loss 0.0222, time 841.85ms, mfu 39.31%
iter 7170: loss 0.0217, time 839.90ms, mfu 39.39%
iter 7180: loss 0.0233, time 842.16ms, mfu 39.45%
iter 7190: loss 0.0202, time 841.24ms, mfu 39.50%
iter 7200: loss 0.0245, time 840.85ms, mfu 39.56%
iter 7210: loss 0.0218, time 840.08ms, mfu 39.61%
iter 7220: loss 0.0212, time 840.66ms, mfu 39.65%
iter 7230: loss 0.0231, time 840.99ms, mfu 39.69%
iter 7240: loss 0.0286, time 840.24ms, mfu 39.73%
step 7250: train loss 0.0220, val loss 8.7221
saving checkpoint to out-whatsapp
iter 7250: loss 0.0181, time 34512.34ms, mfu 35.85%
iter 7260: loss 0.0243, time 837.79ms, mfu 36.29%
