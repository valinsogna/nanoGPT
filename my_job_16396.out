master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Overriding config with config/train_whatsapp.py:Overriding config with config/train_whatsapp.py:

# config for training GPT-2 (124M) down to very nice loss of ~2.85 on 1 node of 8X A100 40GB
# launch as the following (e.g. in a screen session) and wait ~5 days:
# $ torchrun --standalone --nproc_per_node=8 train.py config/train_whatsapp.py

#####################################################################
# CHANGED default parameters from train.py model:
#####################################################################
out_dir = 'out-whatsapp'
dataset = 'whatsapp'
wandb_log = False
wandb_project = 'whatsapp'
wandb_run_name='gpt2-124M'

# bias = False
min_lr = 1e-4 # learning_rate / 10 usually
beta2 = 0.99 # make a bit bigger because number of tokens per iter is small

# eval stuff
eval_interval = 250 # keep frequent because we'll overfit
eval_iters = 200
log_interval = 10 # don't print too too often

#####################################################################
# UNCHANGED default parameters from train.py model:
#####################################################################
# these make the total batch size be ~0.5M (recalculate for whatsapp)
# 12 batch size * 1024 block size * 5 gradaccum * 8 GPUs = 491,520 (recalculate for whatsapp)
batch_size = 12
block_size = 1024
gradient_accumulation_steps = 5 * 8

# this makes total number of tokens be 300B (recalculate for whatsapp)
max_iters = 600000
lr_decay_iters = 600000

# weight decay
weight_decay = 1e-1


#tokens per iteration will be: 491,520
#number of parameters: 123.59M
# config for training GPT-2 (124M) down to very nice loss of ~2.85 on 1 node of 8X A100 40GB
# launch as the following (e.g. in a screen session) and wait ~5 days:
# $ torchrun --standalone --nproc_per_node=8 train.py config/train_whatsapp.py

#####################################################################
# CHANGED default parameters from train.py model:
#####################################################################
out_dir = 'out-whatsapp'
dataset = 'whatsapp'
wandb_log = False
wandb_project = 'whatsapp'
wandb_run_name='gpt2-124M'

# bias = False
min_lr = 1e-4 # learning_rate / 10 usually
beta2 = 0.99 # make a bit bigger because number of tokens per iter is small

# eval stuff
eval_interval = 250 # keep frequent because we'll overfit
eval_iters = 200
log_interval = 10 # don't print too too often

#####################################################################
# UNCHANGED default parameters from train.py model:
#####################################################################
# these make the total batch size be ~0.5M (recalculate for whatsapp)
# 12 batch size * 1024 block size * 5 gradaccum * 8 GPUs = 491,520 (recalculate for whatsapp)
batch_size = 12
block_size = 1024
gradient_accumulation_steps = 5 * 8

# this makes total number of tokens be 300B (recalculate for whatsapp)
max_iters = 600000
lr_decay_iters = 600000

# weight decay
weight_decay = 1e-1


#tokens per iteration will be: 491,520
#number of parameters: 123.59M
Overriding config with config/train_whatsapp.py:
# config for training GPT-2 (124M) down to very nice loss of ~2.85 on 1 node of 8X A100 40GB
# launch as the following (e.g. in a screen session) and wait ~5 days:
# $ torchrun --standalone --nproc_per_node=8 train.py config/train_whatsapp.py

#####################################################################
# CHANGED default parameters from train.py model:
#####################################################################
out_dir = 'out-whatsapp'
dataset = 'whatsapp'
wandb_log = False
wandb_project = 'whatsapp'
wandb_run_name='gpt2-124M'

# bias = False
min_lr = 1e-4 # learning_rate / 10 usually
beta2 = 0.99 # make a bit bigger because number of tokens per iter is small

# eval stuff
eval_interval = 250 # keep frequent because we'll overfit
eval_iters = 200
log_interval = 10 # don't print too too often

#####################################################################
# UNCHANGED default parameters from train.py model:
#####################################################################
# these make the total batch size be ~0.5M (recalculate for whatsapp)
# 12 batch size * 1024 block size * 5 gradaccum * 8 GPUs = 491,520 (recalculate for whatsapp)
batch_size = 12
block_size = 1024
gradient_accumulation_steps = 5 * 8

# this makes total number of tokens be 300B (recalculate for whatsapp)
max_iters = 600000
lr_decay_iters = 600000

# weight decay
weight_decay = 1e-1


#tokens per iteration will be: 491,520
#number of parameters: 123.59M
Overriding config with config/train_whatsapp.py:
# config for training GPT-2 (124M) down to very nice loss of ~2.85 on 1 node of 8X A100 40GB
# launch as the following (e.g. in a screen session) and wait ~5 days:
# $ torchrun --standalone --nproc_per_node=8 train.py config/train_whatsapp.py

#####################################################################
# CHANGED default parameters from train.py model:
#####################################################################
out_dir = 'out-whatsapp'
dataset = 'whatsapp'
wandb_log = False
wandb_project = 'whatsapp'
wandb_run_name='gpt2-124M'

# bias = False
min_lr = 1e-4 # learning_rate / 10 usually
beta2 = 0.99 # make a bit bigger because number of tokens per iter is small

# eval stuff
eval_interval = 250 # keep frequent because we'll overfit
eval_iters = 200
log_interval = 10 # don't print too too often

#####################################################################
# UNCHANGED default parameters from train.py model:
#####################################################################
# these make the total batch size be ~0.5M (recalculate for whatsapp)
# 12 batch size * 1024 block size * 5 gradaccum * 8 GPUs = 491,520 (recalculate for whatsapp)
batch_size = 12
block_size = 1024
gradient_accumulation_steps = 5 * 8

# this makes total number of tokens be 300B (recalculate for whatsapp)
max_iters = 600000
lr_decay_iters = 600000

# weight decay
weight_decay = 1e-1


#tokens per iteration will be: 491,520
#number of parameters: 123.59M
tokens per iteration will be: 491,520
tokens per iteration will be: 491,520tokens per iteration will be: 491,520

tokens per iteration will be: 491,520
Initializing a new model from scratch
Initializing a new model from scratchInitializing a new model from scratchdefaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)
Initializing a new model from scratch

defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)
defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)
defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)

number of parameters: 123.59M
number of parameters: 123.59M
number of parameters: 123.59M
number of parameters: 123.59M
num decayed parameter tensors: 50, with 124,354,560 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
using fused AdamW: True
compiling the model... (takes a ~minute)
num decayed parameter tensors: 50, with 124,354,560 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
using fused AdamW: True
compiling the model... (takes a ~minute)
num decayed parameter tensors: 50, with 124,354,560 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
using fused AdamW: True
compiling the model... (takes a ~minute)
num decayed parameter tensors: 50, with 124,354,560 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
using fused AdamW: True
compiling the model... (takes a ~minute)
/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.3) or chardet (5.1.0) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.3) or chardet (5.1.0) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.3) or chardet (5.1.0) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.3) or chardet (5.1.0) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
step 0: train loss 10.9786, val loss 10.9765
iter 0: loss 10.9807, time 34819.17ms, mfu -100.00%
iter 10: loss 10.2626, time 835.46ms, mfu 40.29%
iter 20: loss 9.2857, time 835.31ms, mfu 40.30%
iter 30: loss 8.9212, time 837.02ms, mfu 40.29%
iter 40: loss 8.3362, time 838.05ms, mfu 40.28%
iter 50: loss 7.6895, time 837.76ms, mfu 40.27%
iter 60: loss 7.3525, time 838.64ms, mfu 40.25%
iter 70: loss 6.8220, time 838.38ms, mfu 40.24%
iter 80: loss 6.5586, time 838.47ms, mfu 40.24%
iter 90: loss 6.5405, time 839.38ms, mfu 40.22%
iter 100: loss 6.0499, time 839.77ms, mfu 40.21%
iter 110: loss 6.0185, time 839.22ms, mfu 40.20%
iter 120: loss 5.5227, time 839.40ms, mfu 40.19%
iter 130: loss 5.2670, time 839.22ms, mfu 40.18%
iter 140: loss 5.0093, time 838.97ms, mfu 40.18%
iter 150: loss 4.7852, time 839.87ms, mfu 40.17%
iter 160: loss 4.6014, time 839.37ms, mfu 40.16%
iter 170: loss 4.0930, time 839.06ms, mfu 40.16%
iter 180: loss 4.3871, time 840.11ms, mfu 40.15%
iter 190: loss 3.9806, time 840.99ms, mfu 40.14%
iter 200: loss 3.9256, time 842.02ms, mfu 40.12%
iter 210: loss 3.3142, time 839.88ms, mfu 40.12%
iter 220: loss 3.5063, time 839.86ms, mfu 40.11%
iter 230: loss 3.5292, time 841.02ms, mfu 40.11%
iter 240: loss 3.3870, time 840.00ms, mfu 40.10%
step 250: train loss 3.4197, val loss 4.2895
saving checkpoint to out-whatsapp
iter 250: loss 3.1609, time 11662.01ms, mfu 36.38%
iter 260: loss 3.2681, time 838.06ms, mfu 36.76%
iter 270: loss 3.1930, time 839.40ms, mfu 37.09%
iter 280: loss 3.4704, time 841.15ms, mfu 37.39%
iter 290: loss 3.2240, time 840.13ms, mfu 37.66%
iter 300: loss 3.3175, time 840.75ms, mfu 37.89%
iter 310: loss 3.1754, time 839.65ms, mfu 38.11%
iter 320: loss 3.0036, time 841.29ms, mfu 38.30%
iter 330: loss 3.1429, time 842.19ms, mfu 38.47%
iter 340: loss 3.1980, time 841.06ms, mfu 38.63%
iter 350: loss 2.9396, time 839.46ms, mfu 38.77%
iter 360: loss 3.0103, time 841.42ms, mfu 38.90%
iter 370: loss 3.1691, time 840.07ms, mfu 39.02%
iter 380: loss 2.8426, time 841.57ms, mfu 39.11%
iter 390: loss 2.8958, time 842.24ms, mfu 39.20%
iter 400: loss 2.8318, time 841.06ms, mfu 39.28%
iter 410: loss 2.7530, time 840.68ms, mfu 39.36%
iter 420: loss 2.5889, time 840.24ms, mfu 39.43%
iter 430: loss 2.9440, time 840.27ms, mfu 39.49%
iter 440: loss 2.6920, time 840.80ms, mfu 39.55%
iter 450: loss 2.5605, time 842.12ms, mfu 39.59%
iter 460: loss 2.6536, time 841.80ms, mfu 39.63%
iter 470: loss 2.5595, time 841.35ms, mfu 39.67%
iter 480: loss 2.9449, time 841.55ms, mfu 39.70%
iter 490: loss 2.6315, time 842.32ms, mfu 39.73%
step 500: train loss 2.5501, val loss 3.5086
saving checkpoint to out-whatsapp
iter 500: loss 2.5356, time 11650.11ms, mfu 36.04%
iter 510: loss 2.5027, time 839.42ms, mfu 36.45%
iter 520: loss 2.4596, time 840.20ms, mfu 36.81%
iter 530: loss 2.3043, time 839.36ms, mfu 37.14%
iter 540: loss 2.4416, time 840.87ms, mfu 37.43%
iter 550: loss 2.1939, time 839.61ms, mfu 37.70%
iter 560: loss 2.4246, time 839.34ms, mfu 37.94%
iter 570: loss 2.4088, time 839.79ms, mfu 38.15%
iter 580: loss 2.3598, time 842.31ms, mfu 38.34%
iter 590: loss 2.3005, time 840.77ms, mfu 38.51%
iter 600: loss 2.1852, time 840.53ms, mfu 38.66%
iter 610: loss 2.2066, time 841.89ms, mfu 38.79%
iter 620: loss 2.1570, time 842.04ms, mfu 38.91%
iter 630: loss 2.2322, time 840.47ms, mfu 39.03%
iter 640: loss 2.2303, time 840.70ms, mfu 39.13%
iter 650: loss 2.2287, time 842.15ms, mfu 39.21%
iter 660: loss 2.0955, time 842.46ms, mfu 39.29%
iter 670: loss 2.1543, time 839.66ms, mfu 39.37%
iter 680: loss 1.9711, time 840.18ms, mfu 39.44%
iter 690: loss 2.0168, time 842.67ms, mfu 39.49%
iter 700: loss 1.9668, time 841.96ms, mfu 39.54%
iter 710: loss 1.9864, time 842.19ms, mfu 39.58%
iter 720: loss 1.7350, time 840.78ms, mfu 39.63%
iter 730: loss 1.6807, time 842.53ms, mfu 39.66%
iter 740: loss 1.7794, time 842.11ms, mfu 39.69%
step 750: train loss 1.7479, val loss 3.4929
saving checkpoint to out-whatsapp
iter 750: loss 1.6914, time 11778.46ms, mfu 36.01%
iter 760: loss 1.7207, time 840.88ms, mfu 36.41%
iter 770: loss 1.6203, time 842.03ms, mfu 36.77%
iter 780: loss 1.5569, time 842.17ms, mfu 37.09%
iter 790: loss 1.5727, time 839.72ms, mfu 37.39%
iter 800: loss 1.3225, time 842.04ms, mfu 37.65%
iter 810: loss 1.3645, time 842.37ms, mfu 37.88%
iter 820: loss 1.4007, time 842.05ms, mfu 38.09%
iter 830: loss 1.3486, time 842.12ms, mfu 38.28%
iter 840: loss 1.3837, time 841.62ms, mfu 38.45%
iter 850: loss 1.2044, time 842.13ms, mfu 38.60%
iter 860: loss 1.1192, time 841.71ms, mfu 38.74%
iter 870: loss 1.0633, time 841.30ms, mfu 38.87%
iter 880: loss 0.9932, time 842.53ms, mfu 38.98%
iter 890: loss 1.0328, time 840.75ms, mfu 39.08%
iter 900: loss 0.8162, time 846.90ms, mfu 39.15%
iter 910: loss 0.8789, time 842.36ms, mfu 39.23%
iter 920: loss 0.7988, time 841.59ms, mfu 39.31%
iter 930: loss 0.8185, time 841.27ms, mfu 39.38%
iter 940: loss 0.7057, time 848.69ms, mfu 39.41%
iter 950: loss 0.6730, time 842.79ms, mfu 39.46%
iter 960: loss 0.6735, time 840.86ms, mfu 39.52%
iter 970: loss 0.5834, time 841.26ms, mfu 39.57%
iter 980: loss 0.4974, time 841.33ms, mfu 39.61%
iter 990: loss 0.4791, time 842.08ms, mfu 39.65%
step 1000: train loss 0.4662, val loss 4.5907
saving checkpoint to out-whatsapp
iter 1000: loss 0.4535, time 11726.64ms, mfu 35.97%
iter 1010: loss 0.4650, time 839.10ms, mfu 36.39%
iter 1020: loss 0.4461, time 841.55ms, mfu 36.75%
iter 1030: loss 0.3663, time 842.23ms, mfu 37.07%
iter 1040: loss 0.3587, time 841.14ms, mfu 37.37%
iter 1050: loss 0.2914, time 841.68ms, mfu 37.63%
iter 1060: loss 0.3150, time 842.11ms, mfu 37.86%
iter 1070: loss 0.2417, time 842.42ms, mfu 38.07%
iter 1080: loss 0.2356, time 842.28ms, mfu 38.26%
iter 1090: loss 0.2433, time 842.18ms, mfu 38.43%
iter 1100: loss 0.2240, time 841.98ms, mfu 38.59%
iter 1110: loss 0.2222, time 840.77ms, mfu 38.73%
iter 1120: loss 0.2273, time 841.56ms, mfu 38.86%
iter 1130: loss 0.2149, time 841.37ms, mfu 38.98%
iter 1140: loss 0.3293, time 841.86ms, mfu 39.08%
iter 1150: loss 0.2556, time 841.99ms, mfu 39.17%
iter 1160: loss 0.7782, time 841.66ms, mfu 39.25%
iter 1170: loss 5.7645, time 845.76ms, mfu 39.31%
iter 1180: loss 3.6350, time 840.46ms, mfu 39.38%
iter 1190: loss 2.6764, time 841.18ms, mfu 39.45%
iter 1200: loss 2.3629, time 841.84ms, mfu 39.50%
iter 1210: loss 2.1662, time 841.73ms, mfu 39.55%
iter 1220: loss 2.0649, time 840.60ms, mfu 39.60%
iter 1230: loss 1.6075, time 841.65ms, mfu 39.64%
iter 1240: loss 1.7036, time 842.27ms, mfu 39.67%
step 1250: train loss 1.3459, val loss 3.9747
saving checkpoint to out-whatsapp
iter 1250: loss 1.4343, time 11722.71ms, mfu 35.99%
iter 1260: loss 1.1994, time 838.31ms, mfu 36.41%
iter 1270: loss 0.8660, time 841.08ms, mfu 36.77%
iter 1280: loss 0.6574, time 840.50ms, mfu 37.10%
iter 1290: loss 0.3753, time 840.87ms, mfu 37.39%
iter 1300: loss 0.2782, time 840.89ms, mfu 37.66%
iter 1310: loss 0.2229, time 841.82ms, mfu 37.89%
iter 1320: loss 0.1452, time 840.47ms, mfu 38.11%
iter 1330: loss 0.1733, time 842.63ms, mfu 38.29%
iter 1340: loss 0.1454, time 843.97ms, mfu 38.45%
iter 1350: loss 0.1559, time 843.05ms, mfu 38.60%
iter 1360: loss 0.1614, time 842.37ms, mfu 38.74%
iter 1370: loss 0.1510, time 842.43ms, mfu 38.86%
iter 1380: loss 0.1121, time 841.98ms, mfu 38.97%
iter 1390: loss 0.1462, time 841.97ms, mfu 39.07%
iter 1400: loss 0.0953, time 842.92ms, mfu 39.16%
iter 1410: loss 0.0976, time 841.75ms, mfu 39.24%
iter 1420: loss 0.1067, time 842.65ms, mfu 39.31%
iter 1430: loss 0.0904, time 842.15ms, mfu 39.38%
iter 1440: loss 0.1003, time 841.16ms, mfu 39.44%
iter 1450: loss 0.0973, time 842.56ms, mfu 39.49%
iter 1460: loss 0.0878, time 841.28ms, mfu 39.55%
iter 1470: loss 0.0972, time 840.89ms, mfu 39.60%
iter 1480: loss 0.0756, time 841.86ms, mfu 39.63%
iter 1490: loss 0.0886, time 841.86ms, mfu 39.67%
step 1500: train loss 0.1047, val loss 5.4705
saving checkpoint to out-whatsapp
iter 1500: loss 0.1034, time 11726.83ms, mfu 35.99%
iter 1510: loss 0.1776, time 839.93ms, mfu 36.40%
iter 1520: loss 0.1138, time 840.00ms, mfu 36.77%
iter 1530: loss 0.1011, time 841.95ms, mfu 37.09%
iter 1540: loss 0.0942, time 840.63ms, mfu 37.38%
iter 1550: loss 0.0817, time 840.57ms, mfu 37.65%
iter 1560: loss 0.0636, time 840.58ms, mfu 37.89%
iter 1570: loss 0.0621, time 842.49ms, mfu 38.10%
iter 1580: loss 0.0682, time 841.39ms, mfu 38.29%
iter 1590: loss 0.0772, time 840.81ms, mfu 38.46%
iter 1600: loss 0.0734, time 842.52ms, mfu 38.61%
iter 1610: loss 0.1681, time 842.24ms, mfu 38.75%
iter 1620: loss 0.3206, time 840.70ms, mfu 38.88%
iter 1630: loss 0.4458, time 842.28ms, mfu 38.99%
iter 1640: loss 0.6007, time 843.90ms, mfu 39.08%
iter 1650: loss 2.8966, time 841.24ms, mfu 39.17%
iter 1660: loss 6.2988, time 840.36ms, mfu 39.26%
iter 1670: loss 4.4827, time 841.93ms, mfu 39.33%
iter 1680: loss 3.1527, time 841.04ms, mfu 39.40%
iter 1690: loss 2.9063, time 841.71ms, mfu 39.46%
iter 1700: loss 2.4903, time 841.92ms, mfu 39.51%
iter 1710: loss 2.2948, time 842.19ms, mfu 39.56%
iter 1720: loss 2.1143, time 842.47ms, mfu 39.60%
iter 1730: loss 1.7877, time 842.61ms, mfu 39.64%
iter 1740: loss 1.4424, time 841.34ms, mfu 39.67%
step 1750: train loss 1.0810, val loss 4.1790
saving checkpoint to out-whatsapp
iter 1750: loss 1.0709, time 11725.38ms, mfu 35.99%
iter 1760: loss 0.7990, time 839.03ms, mfu 36.41%
iter 1770: loss 0.4791, time 838.82ms, mfu 36.78%
iter 1780: loss 0.4702, time 841.66ms, mfu 37.10%
iter 1790: loss 0.3027, time 840.38ms, mfu 37.40%
iter 1800: loss 0.1409, time 841.47ms, mfu 37.66%
iter 1810: loss 0.0886, time 841.61ms, mfu 37.89%
iter 1820: loss 0.0608, time 842.25ms, mfu 38.10%
iter 1830: loss 0.0545, time 841.05ms, mfu 38.29%
iter 1840: loss 0.0611, time 841.30ms, mfu 38.46%
iter 1850: loss 0.0693, time 843.33ms, mfu 38.61%
iter 1860: loss 0.0594, time 841.53ms, mfu 38.75%
iter 1870: loss 0.0581, time 842.04ms, mfu 38.87%
iter 1880: loss 0.0650, time 840.96ms, mfu 38.99%
iter 1890: loss 0.0579, time 842.11ms, mfu 39.09%
iter 1900: loss 0.0634, time 841.66ms, mfu 39.18%
iter 1910: loss 0.0579, time 843.19ms, mfu 39.25%
iter 1920: loss 0.0606, time 843.30ms, mfu 39.32%
iter 1930: loss 0.0713, time 841.25ms, mfu 39.39%
iter 1940: loss 0.0618, time 841.30ms, mfu 39.45%
iter 1950: loss 0.1670, time 843.45ms, mfu 39.50%
iter 1960: loss 0.1281, time 842.32ms, mfu 39.55%
iter 1970: loss 0.0876, time 842.05ms, mfu 39.59%
iter 1980: loss 0.0762, time 842.47ms, mfu 39.63%
iter 1990: loss 0.0698, time 841.79ms, mfu 39.66%
step 2000: train loss 0.0680, val loss 5.8644
saving checkpoint to out-whatsapp
iter 2000: loss 0.0598, time 11733.41ms, mfu 35.98%
iter 2010: loss 0.0780, time 839.43ms, mfu 36.40%
iter 2020: loss 0.0717, time 840.44ms, mfu 36.76%
iter 2030: loss 0.0655, time 841.43ms, mfu 37.09%
iter 2040: loss 0.0654, time 840.95ms, mfu 37.38%
iter 2050: loss 0.0715, time 842.99ms, mfu 37.64%
iter 2060: loss 0.0578, time 842.95ms, mfu 37.87%
iter 2070: loss 0.0631, time 842.59ms, mfu 38.07%
iter 2080: loss 0.0604, time 841.30ms, mfu 38.27%
iter 2090: loss 0.0692, time 841.53ms, mfu 38.44%
iter 2100: loss 0.1390, time 841.64ms, mfu 38.60%
iter 2110: loss 0.1805, time 841.81ms, mfu 38.74%
iter 2120: loss 0.1122, time 841.84ms, mfu 38.86%
iter 2130: loss 0.0815, time 841.70ms, mfu 38.98%
iter 2140: loss 0.0654, time 841.93ms, mfu 39.08%
iter 2150: loss 0.0675, time 840.92ms, mfu 39.17%
iter 2160: loss 0.0675, time 842.50ms, mfu 39.25%
iter 2170: loss 0.0604, time 843.50ms, mfu 39.32%
iter 2180: loss 0.0495, time 841.84ms, mfu 39.38%
iter 2190: loss 0.0499, time 842.03ms, mfu 39.44%
iter 2200: loss 0.0416, time 841.40ms, mfu 39.50%
iter 2210: loss 0.0523, time 841.93ms, mfu 39.55%
iter 2220: loss 0.0517, time 842.55ms, mfu 39.59%
iter 2230: loss 0.0483, time 842.01ms, mfu 39.63%
iter 2240: loss 0.0558, time 842.05ms, mfu 39.66%
step 2250: train loss 0.0529, val loss 6.2481
saving checkpoint to out-whatsapp
iter 2250: loss 0.0534, time 11731.46ms, mfu 35.98%
iter 2260: loss 0.0548, time 839.65ms, mfu 36.40%
iter 2270: loss 0.0577, time 841.81ms, mfu 36.76%
iter 2280: loss 0.0611, time 842.14ms, mfu 37.08%
iter 2290: loss 0.0642, time 840.61ms, mfu 37.37%
iter 2300: loss 0.0562, time 841.03ms, mfu 37.64%
iter 2310: loss 0.2799, time 840.25ms, mfu 37.88%
iter 2320: loss 0.5055, time 841.03ms, mfu 38.10%
iter 2330: loss 0.3418, time 841.55ms, mfu 38.29%
iter 2340: loss 2.4594, time 840.70ms, mfu 38.46%
iter 2350: loss 2.1152, time 842.34ms, mfu 38.61%
iter 2360: loss 1.7093, time 840.10ms, mfu 38.76%
iter 2370: loss 1.1816, time 842.12ms, mfu 38.88%
iter 2380: loss 0.8811, time 842.63ms, mfu 38.99%
iter 2390: loss 0.3555, time 841.58ms, mfu 39.09%
iter 2400: loss 0.1247, time 842.00ms, mfu 39.18%
iter 2410: loss 0.0811, time 841.64ms, mfu 39.26%
iter 2420: loss 0.0506, time 841.94ms, mfu 39.33%
iter 2430: loss 0.0431, time 841.91ms, mfu 39.40%
iter 2440: loss 0.0412, time 842.66ms, mfu 39.45%
iter 2450: loss 0.0369, time 841.18ms, mfu 39.51%
iter 2460: loss 0.0334, time 841.84ms, mfu 39.56%
iter 2470: loss 0.0403, time 841.57ms, mfu 39.60%
iter 2480: loss 0.0378, time 842.78ms, mfu 39.64%
iter 2490: loss 0.0329, time 841.96ms, mfu 39.67%
step 2500: train loss 0.0365, val loss 6.3056
saving checkpoint to out-whatsapp
iter 2500: loss 0.0435, time 11731.78ms, mfu 35.99%
iter 2510: loss 0.0449, time 838.45ms, mfu 36.41%
iter 2520: loss 0.0370, time 838.80ms, mfu 36.78%
iter 2530: loss 0.0348, time 843.06ms, mfu 37.10%
iter 2540: loss 0.0391, time 841.18ms, mfu 37.39%
iter 2550: loss 0.0378, time 840.70ms, mfu 37.65%
iter 2560: loss 0.0336, time 841.30ms, mfu 37.89%
iter 2570: loss 0.0396, time 841.63ms, mfu 38.10%
iter 2580: loss 0.0479, time 841.92ms, mfu 38.29%
iter 2590: loss 0.0438, time 841.22ms, mfu 38.46%
iter 2600: loss 0.0367, time 841.42ms, mfu 38.62%
iter 2610: loss 0.0358, time 842.71ms, mfu 38.75%
iter 2620: loss 0.0418, time 841.24ms, mfu 38.88%
iter 2630: loss 0.0431, time 841.90ms, mfu 38.99%
iter 2640: loss 0.0419, time 841.10ms, mfu 39.09%
iter 2650: loss 0.0488, time 840.98ms, mfu 39.19%
iter 2660: loss 0.0508, time 840.96ms, mfu 39.27%
iter 2670: loss 0.0513, time 842.25ms, mfu 39.34%
iter 2680: loss 0.0439, time 841.55ms, mfu 39.41%
iter 2690: loss 0.0451, time 842.52ms, mfu 39.46%
iter 2700: loss 0.0591, time 841.78ms, mfu 39.51%
iter 2710: loss 0.1639, time 841.75ms, mfu 39.56%
iter 2720: loss 0.2461, time 843.05ms, mfu 39.60%
iter 2730: loss 0.8389, time 841.42ms, mfu 39.64%
iter 2740: loss 1.5407, time 840.46ms, mfu 39.68%
step 2750: train loss 2.0456, val loss 4.4351
saving checkpoint to out-whatsapp
iter 2750: loss 2.1999, time 11725.35ms, mfu 36.00%
iter 2760: loss 1.9137, time 838.38ms, mfu 36.42%
iter 2770: loss 1.2455, time 840.55ms, mfu 36.78%
iter 2780: loss 0.3263, time 841.93ms, mfu 37.10%
iter 2790: loss 0.0938, time 841.18ms, mfu 37.39%
iter 2800: loss 0.0642, time 840.91ms, mfu 37.66%
iter 2810: loss 0.0418, time 841.03ms, mfu 37.89%
iter 2820: loss 0.0410, time 840.09ms, mfu 38.11%
iter 2830: loss 0.0366, time 841.80ms, mfu 38.30%
iter 2840: loss 0.0292, time 840.88ms, mfu 38.47%
iter 2850: loss 0.0369, time 842.47ms, mfu 38.62%
iter 2860: loss 0.0360, time 842.31ms, mfu 38.76%
iter 2870: loss 0.0336, time 842.10ms, mfu 38.88%
iter 2880: loss 0.0406, time 841.89ms, mfu 38.99%
iter 2890: loss 0.0337, time 843.74ms, mfu 39.08%
iter 2900: loss 0.0351, time 843.22ms, mfu 39.16%
iter 2910: loss 0.0310, time 842.02ms, mfu 39.25%
iter 2920: loss 0.0366, time 841.49ms, mfu 39.32%
iter 2930: loss 0.0297, time 841.63ms, mfu 39.39%
iter 2940: loss 0.0313, time 842.48ms, mfu 39.45%
iter 2950: loss 0.0338, time 842.55ms, mfu 39.50%
iter 2960: loss 0.0374, time 842.41ms, mfu 39.54%
iter 2970: loss 0.0366, time 840.92ms, mfu 39.59%
iter 2980: loss 0.0323, time 841.84ms, mfu 39.63%
iter 2990: loss 0.0318, time 842.86ms, mfu 39.66%
step 3000: train loss 0.0349, val loss 6.4920
saving checkpoint to out-whatsapp
iter 3000: loss 0.0321, time 11776.46ms, mfu 35.98%
iter 3010: loss 0.0416, time 839.24ms, mfu 36.40%
iter 3020: loss 0.0403, time 842.27ms, mfu 36.75%
iter 3030: loss 0.0402, time 841.04ms, mfu 37.08%
iter 3040: loss 0.0417, time 842.69ms, mfu 37.37%
iter 3050: loss 0.0474, time 841.12ms, mfu 37.63%
iter 3060: loss 0.0526, time 841.16ms, mfu 37.87%
iter 3070: loss 0.0458, time 841.12ms, mfu 38.09%
iter 3080: loss 0.0457, time 841.99ms, mfu 38.28%
iter 3090: loss 0.0454, time 841.57ms, mfu 38.45%
iter 3100: loss 0.0453, time 842.51ms, mfu 38.60%
iter 3110: loss 0.0473, time 842.53ms, mfu 38.74%
iter 3120: loss 0.0414, time 842.17ms, mfu 38.86%
iter 3130: loss 0.0416, time 841.76ms, mfu 38.97%
iter 3140: loss 0.0420, time 842.68ms, mfu 39.07%
iter 3150: loss 0.0422, time 842.12ms, mfu 39.16%
iter 3160: loss 0.0418, time 840.75ms, mfu 39.25%
iter 3170: loss 0.0432, time 841.69ms, mfu 39.32%
iter 3180: loss 0.0413, time 842.44ms, mfu 39.39%
iter 3190: loss 0.0456, time 841.93ms, mfu 39.45%
iter 3200: loss 0.0426, time 840.88ms, mfu 39.51%
iter 3210: loss 0.0455, time 842.32ms, mfu 39.55%
iter 3220: loss 0.0443, time 841.58ms, mfu 39.60%
iter 3230: loss 0.0406, time 842.29ms, mfu 39.63%
iter 3240: loss 0.0483, time 844.57ms, mfu 39.66%
step 3250: train loss 0.0422, val loss 6.7655
saving checkpoint to out-whatsapp
iter 3250: loss 0.0375, time 11733.52ms, mfu 35.98%
iter 3260: loss 0.0350, time 839.47ms, mfu 36.39%
iter 3270: loss 0.0504, time 839.77ms, mfu 36.76%
iter 3280: loss 0.0429, time 839.62ms, mfu 37.09%
iter 3290: loss 0.0465, time 838.74ms, mfu 37.40%
iter 3300: loss 0.0366, time 842.04ms, mfu 37.66%
iter 3310: loss 0.0418, time 843.26ms, mfu 37.88%
iter 3320: loss 0.0544, time 842.01ms, mfu 38.09%
iter 3330: loss 0.0366, time 840.67ms, mfu 38.29%
iter 3340: loss 0.0448, time 840.98ms, mfu 38.46%
iter 3350: loss 0.0463, time 842.92ms, mfu 38.61%
iter 3360: loss 0.0401, time 844.11ms, mfu 38.74%
iter 3370: loss 0.0329, time 840.98ms, mfu 38.87%
iter 3380: loss 0.0400, time 842.14ms, mfu 38.98%
iter 3390: loss 0.0364, time 841.68ms, mfu 39.08%
iter 3400: loss 0.0345, time 841.77ms, mfu 39.17%
iter 3410: loss 0.0394, time 841.99ms, mfu 39.25%
iter 3420: loss 0.0341, time 841.83ms, mfu 39.33%
iter 3430: loss 0.0407, time 841.78ms, mfu 39.39%
iter 3440: loss 0.0356, time 841.66ms, mfu 39.45%
iter 3450: loss 0.0410, time 842.09ms, mfu 39.51%
iter 3460: loss 0.0382, time 841.56ms, mfu 39.56%
iter 3470: loss 0.0402, time 841.29ms, mfu 39.60%
iter 3480: loss 0.0387, time 842.11ms, mfu 39.64%
iter 3490: loss 0.0376, time 841.23ms, mfu 39.68%
step 3500: train loss 0.0381, val loss 7.0423
saving checkpoint to out-whatsapp
