master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Overriding config with config/train_whatsapp.py:
Overriding config with config/train_whatsapp.py:
Overriding config with config/train_whatsapp.py:
Overriding config with config/train_whatsapp.py:
# config for training GPT-2 (124M) down to very nice loss of ~2.85 on 1 node of 8X A100 40GB
# launch as the following (e.g. in a screen session) and wait ~5 days:
# $ torchrun --standalone --nproc_per_node=8 train.py config/train_gpt2.py

#####################################################################
# CHANGED default parameters from train.py model:
#####################################################################
out_dir = 'out-whatsapp'
dataset = 'whatsapp'
wandb_log = True
wandb_project = 'whatsapp'
wandb_run_name='gpt2-124M'

# bias = False
min_lr = 1e-4 # learning_rate / 10 usually
beta2 = 0.99 # make a bit bigger because number of tokens per iter is small

# eval stuff
eval_interval = 250 # keep frequent because we'll overfit
eval_iters = 200
log_interval = 10 # don't print too too often

#####################################################################
# UNCHANGED default parameters from train.py model:
#####################################################################
# these make the total batch size be ~0.5M (recalculate for whatsapp)
# 12 batch size * 1024 block size * 5 gradaccum * 8 GPUs = 491,520 (recalculate for whatsapp)
batch_size = 12
block_size = 1024
gradient_accumulation_steps = 5 * 8

# this makes total number of tokens be 300B (recalculate for whatsapp)
max_iters = 600000
lr_decay_iters = 600000

# weight decay
weight_decay = 1e-1

# config for training GPT-2 (124M) down to very nice loss of ~2.85 on 1 node of 8X A100 40GB
# launch as the following (e.g. in a screen session) and wait ~5 days:
# $ torchrun --standalone --nproc_per_node=8 train.py config/train_gpt2.py

#####################################################################
# CHANGED default parameters from train.py model:
#####################################################################
out_dir = 'out-whatsapp'
dataset = 'whatsapp'
wandb_log = True
wandb_project = 'whatsapp'
wandb_run_name='gpt2-124M'

# bias = False
min_lr = 1e-4 # learning_rate / 10 usually
beta2 = 0.99 # make a bit bigger because number of tokens per iter is small

# eval stuff
eval_interval = 250 # keep frequent because we'll overfit
eval_iters = 200
log_interval = 10 # don't print too too often

#####################################################################
# UNCHANGED default parameters from train.py model:
#####################################################################
# these make the total batch size be ~0.5M (recalculate for whatsapp)
# 12 batch size * 1024 block size * 5 gradaccum * 8 GPUs = 491,520 (recalculate for whatsapp)
batch_size = 12
block_size = 1024
gradient_accumulation_steps = 5 * 8

# this makes total number of tokens be 300B (recalculate for whatsapp)
max_iters = 600000
lr_decay_iters = 600000

# weight decay
weight_decay = 1e-1
# config for training GPT-2 (124M) down to very nice loss of ~2.85 on 1 node of 8X A100 40GB
# launch as the following (e.g. in a screen session) and wait ~5 days:
# $ torchrun --standalone --nproc_per_node=8 train.py config/train_gpt2.py

#####################################################################
# CHANGED default parameters from train.py model:
#####################################################################
out_dir = 'out-whatsapp'
dataset = 'whatsapp'
wandb_log = True
wandb_project = 'whatsapp'
wandb_run_name='gpt2-124M'

# bias = False
min_lr = 1e-4 # learning_rate / 10 usually
beta2 = 0.99 # make a bit bigger because number of tokens per iter is small

# eval stuff
eval_interval = 250 # keep frequent because we'll overfit
eval_iters = 200
log_interval = 10 # don't print too too often

#####################################################################
# UNCHANGED default parameters from train.py model:
#####################################################################
# these make the total batch size be ~0.5M (recalculate for whatsapp)
# 12 batch size * 1024 block size * 5 gradaccum * 8 GPUs = 491,520 (recalculate for whatsapp)
batch_size = 12
block_size = 1024
gradient_accumulation_steps = 5 * 8

# this makes total number of tokens be 300B (recalculate for whatsapp)
max_iters = 600000
lr_decay_iters = 600000

# weight decay
weight_decay = 1e-1

# config for training GPT-2 (124M) down to very nice loss of ~2.85 on 1 node of 8X A100 40GB
# launch as the following (e.g. in a screen session) and wait ~5 days:
# $ torchrun --standalone --nproc_per_node=8 train.py config/train_gpt2.py

#####################################################################
# CHANGED default parameters from train.py model:
#####################################################################
out_dir = 'out-whatsapp'
dataset = 'whatsapp'
wandb_log = True
wandb_project = 'whatsapp'
wandb_run_name='gpt2-124M'

# bias = False
min_lr = 1e-4 # learning_rate / 10 usually
beta2 = 0.99 # make a bit bigger because number of tokens per iter is small

# eval stuff
eval_interval = 250 # keep frequent because we'll overfit
eval_iters = 200
log_interval = 10 # don't print too too often

#####################################################################
# UNCHANGED default parameters from train.py model:
#####################################################################
# these make the total batch size be ~0.5M (recalculate for whatsapp)
# 12 batch size * 1024 block size * 5 gradaccum * 8 GPUs = 491,520 (recalculate for whatsapp)
batch_size = 12
block_size = 1024
gradient_accumulation_steps = 5 * 8

# this makes total number of tokens be 300B (recalculate for whatsapp)
max_iters = 600000
lr_decay_iters = 600000

# weight decay
weight_decay = 1e-1


tokens per iteration will be: 491,520
tokens per iteration will be: 491,520
tokens per iteration will be: 491,520
tokens per iteration will be: 491,520
Initializing a new model from scratchInitializing a new model from scratchInitializing a new model from scratchInitializing a new model from scratch


defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)
defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)
defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)


number of parameters: 123.59M
number of parameters: 123.59M
number of parameters: 123.59M
number of parameters: 123.59M
num decayed parameter tensors: 50, with 124,354,560 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
using fused AdamW: True
compiling the model... (takes a ~minute)
num decayed parameter tensors: 50, with 124,354,560 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
using fused AdamW: True
compiling the model... (takes a ~minute)
num decayed parameter tensors: 50, with 124,354,560 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
using fused AdamW: True
compiling the model... (takes a ~minute)
num decayed parameter tensors: 50, with 124,354,560 parameters
num non-decayed parameter tensors: 25, with 19,200 parameters
using fused AdamW: True
compiling the model... (takes a ~minute)
/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.3) or chardet (5.1.0) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.3) or chardet (5.1.0) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.3) or chardet (5.1.0) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.3) or chardet (5.1.0) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
wandb: ERROR api_key not configured (no-tty). call wandb.login(key=[your_api_key])
Traceback (most recent call last):
  File "train.py", line 244, in <module>
    wandb.init(project=wandb_project, name=wandb_run_name, config=config)
  File "/u/dssc/valinsogna/.local/lib/python3.8/site-packages/wandb/sdk/wandb_init.py", line 1171, in init
    raise e
  File "/u/dssc/valinsogna/.local/lib/python3.8/site-packages/wandb/sdk/wandb_init.py", line 1148, in init
    wi.setup(kwargs)
  File "/u/dssc/valinsogna/.local/lib/python3.8/site-packages/wandb/sdk/wandb_init.py", line 289, in setup
    wandb_login._login(
  File "/u/dssc/valinsogna/.local/lib/python3.8/site-packages/wandb/sdk/wandb_login.py", line 298, in _login
    wlogin.prompt_api_key()
  File "/u/dssc/valinsogna/.local/lib/python3.8/site-packages/wandb/sdk/wandb_login.py", line 228, in prompt_api_key
    raise UsageError("api_key not configured (no-tty). call " + directive)
wandb.errors.UsageError: api_key not configured (no-tty). call wandb.login(key=[your_api_key])
Traceback (most recent call last):
  File "train.py", line 244, in <module>
    wandb.init(project=wandb_project, name=wandb_run_name, config=config)
  File "/u/dssc/valinsogna/.local/lib/python3.8/site-packages/wandb/sdk/wandb_init.py", line 1171, in init
    raise e
  File "/u/dssc/valinsogna/.local/lib/python3.8/site-packages/wandb/sdk/wandb_init.py", line 1148, in init
    wi.setup(kwargs)
  File "/u/dssc/valinsogna/.local/lib/python3.8/site-packages/wandb/sdk/wandb_init.py", line 289, in setup
    wandb_login._login(
  File "/u/dssc/valinsogna/.local/lib/python3.8/site-packages/wandb/sdk/wandb_login.py", line 298, in _login
    wlogin.prompt_api_key()
  File "/u/dssc/valinsogna/.local/lib/python3.8/site-packages/wandb/sdk/wandb_login.py", line 228, in prompt_api_key
    raise UsageError("api_key not configured (no-tty). call " + directive)
wandb.errors.UsageError: api_key not configured (no-tty). call wandb.login(key=[your_api_key])
/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.3) or chardet (5.1.0) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 299355 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 299356 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 299357 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 299354) of binary: /usr/bin/python3
/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (2.0.3) or chardet (5.1.0) doesn't match a supported version!
  warnings.warn("urllib3 ({}) or chardet ({}) doesn't match a supported "
Traceback (most recent call last):
  File "/u/dssc/valinsogna//.local/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/u/dssc/valinsogna/.local/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/u/dssc/valinsogna/.local/lib/python3.8/site-packages/torch/distributed/run.py", line 794, in main
    run(args)
  File "/u/dssc/valinsogna/.local/lib/python3.8/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/u/dssc/valinsogna/.local/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/u/dssc/valinsogna/.local/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-07-05_16:35:13
  host      : dgx001.hpc
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 299354)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
